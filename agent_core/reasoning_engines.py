"""
Los 3 motores de razonamiento de M.A.R.T.I.N.
Soporta OpenAI (GPT-4) y Anthropic (Claude)
CON INTEGRACIÃ“N DE TOOLS
"""
from typing import Dict, Any
import os
import sys
from pathlib import Path

# Importar tools
sys.path.insert(0, str(Path(__file__).parent.parent))
try:
    from tools.policy_generator import PolicyGenerator
except ImportError:
    PolicyGenerator = None
    print("âš ï¸ PolicyGenerator no disponible - instala dependencias")

class ReasoningEngines:
    """
    Contiene los 3 modos de razonamiento de M.A.R.T.I.N.
    Soporta mÃºltiples LLMs: OpenAI y Claude
    AHORA CON HERRAMIENTAS REALES
    """
    
    def __init__(self, use_llm: bool = False, llm_provider: str = "auto"):
        """
        Args:
            use_llm: Si True, usa LLM real. Si False, usa respuestas simuladas.
            llm_provider: "openai", "claude", o "auto" (detecta automÃ¡ticamente)
        """
        self.use_llm = use_llm
        self.llm = None
        self.llm_provider = None
        
        if self.use_llm:
            self.llm_provider = self._initialize_llm(llm_provider)
            if not self.llm:
                print("âš ï¸ No se pudo inicializar LLM. Usando modo simulado.")
                self.use_llm = False
        
        # Inicializar herramientas
        if PolicyGenerator:
            self.policy_generator = PolicyGenerator(llm=self.llm if self.use_llm else None)
        else:
            self.policy_generator = None
    
    def _initialize_llm(self, provider: str):
        """Inicializa el LLM segÃºn el proveedor especificado"""
        
        # Auto-detectar quÃ© API key estÃ¡ disponible
        if provider == "auto":
            if os.getenv("ANTHROPIC_API_KEY"):
                provider = "claude"
                print("ğŸ” Auto-detectado: Claude API key disponible")
            elif os.getenv("OPENAI_API_KEY"):
                provider = "openai"
                print("ğŸ” Auto-detectado: OpenAI API key disponible")
            else:
                print("âš ï¸ No se encontrÃ³ OPENAI_API_KEY ni ANTHROPIC_API_KEY")
                return None
        
        # Inicializar OpenAI
        if provider == "openai":
            try:
                from langchain_openai import ChatOpenAI
                api_key = os.getenv("OPENAI_API_KEY")
                
                if not api_key:
                    print("âš ï¸ OPENAI_API_KEY no configurada")
                    return None
                
                self.llm = ChatOpenAI(
                    model="gpt-4",
                    temperature=0,
                    api_key=api_key
                )
                print("âœ… LLM inicializado: OpenAI GPT-4")
                return "openai"
                
            except ImportError:
                print("âš ï¸ langchain no instalado")
                return None
            except Exception as e:
                print(f"âš ï¸ Error inicializando OpenAI: {e}")
                return None
        
        # Inicializar Claude
        elif provider == "claude":
            try:
                from langchain_anthropic import ChatAnthropic
                api_key = os.getenv("ANTHROPIC_API_KEY")
                
                if not api_key:
                    print("âš ï¸ ANTHROPIC_API_KEY no configurada")
                    return None
                
                self.llm = ChatAnthropic(
                    model="claude-3-5-sonnet-20241022",
                    temperature=0,
                    anthropic_api_key=api_key
                )
                print("âœ… LLM inicializado: Anthropic Claude 3.5 Sonnet")
                return "claude"
                
            except ImportError:
                print("âš ï¸ anthropic no instalado. Instala con: pip install anthropic")
                return None
            except Exception as e:
                print(f"âš ï¸ Error inicializando Claude: {e}")
                return None
        
        else:
            print(f"âš ï¸ Proveedor desconocido: {provider}")
            return None
    
    def passive_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO PASIVO: Genera plan pero NO ejecuta
        
        Comportamiento:
        1. Analiza la tarea
        2. Genera un plan detallado
        3. Explica quÃ© harÃ¡
        4. ESPERA confirmaciÃ³n del usuario
        """
        
        if self.use_llm and self.llm:
            prompt = f"""
Eres M.A.R.T.I.N., un agente de IA en MODO PASIVO.

Tu trabajo es:
1. Analizar la tarea del usuario
2. Proponer un plan estructurado
3. Explicar quÃ© harÃ¡s
4. NO ejecutar nada hasta recibir confirmaciÃ³n

Tarea: {task}
Contexto: {context if context else "No hay contexto adicional"}

Responde en este formato:

## ğŸ“‹ MI ANÃLISIS
[CÃ³mo entiendes la tarea]

## ğŸ¯ PLAN PROPUESTO
1. [Paso 1] (tiempo estimado)
2. [Paso 2] (tiempo estimado)

## âš ï¸ CONSIDERACIONES
- [Punto importante 1]
- [Punto importante 2]

Â¿Procedo con este plan?
"""
            try:
                response = self.llm.predict(prompt)
            except Exception as e:
                response = f"Error al llamar LLM: {e}\n"
                response += self._generate_passive_mock(task)
        else:
            response = self._generate_passive_mock(task)
        
        return {
            "mode": "PASSIVE",
            "status": "awaiting_confirmation",
            "plan": response,
            "message": f"ğŸ“‹ MODO PASIVO ACTIVADO\n\n{response}",
            "requires_user_action": True
        }
    
    def direct_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO DIRECTO: Genera plan Y ejecuta automÃ¡ticamente
        
        AHORA CON DETECCIÃ“N Y EJECUCIÃ“N DE HERRAMIENTAS
        """
        
        task_lower = task.lower()
        
        # DETECTAR SI DEBE USAR POLICY GENERATOR
        policy_keywords = ['genera', 'crea', 'escribe', 'crear', 'generar', 'policy', 'polÃ­tica', 'politica']
        policy_types = {
            'password': 'password_policy',
            'contraseÃ±a': 'password_policy',
            'contraseÃ±as': 'password_policy',
            'incidente': 'incident_response',
            'incidentes': 'incident_response',
            'incident': 'incident_response',
            'acceso': 'access_control',
            'access': 'access_control',
            'dato': 'data_classification',
            'datos': 'data_classification',
            'data': 'data_classification',
            'backup': 'backup_recovery',
            'recuperaciÃ³n': 'backup_recovery',
            'recuperacion': 'backup_recovery'
        }
        
        should_generate_policy = any(keyword in task_lower for keyword in policy_keywords)
        detected_policy_type = None
        
        if should_generate_policy:
            for keyword, policy_type in policy_types.items():
                if keyword in task_lower:
                    detected_policy_type = policy_type
                    break
        
        # SI DEBE GENERAR POLÃTICA Y TENEMOS LA TOOL
        if should_generate_policy and detected_policy_type and self.policy_generator:
            
            # Contexto de la empresa
            company_context = {
                'name': context.get('company_name', 'La OrganizaciÃ³n') if context else 'La OrganizaciÃ³n',
                'size': context.get('company_size', '20-50') if context else '20-50',
                'industry': context.get('industry', 'TecnologÃ­a / SaaS') if context else 'TecnologÃ­a / SaaS',
                'tech_stack': context.get('tech_stack', 'Cloud-based') if context else 'Cloud-based',
                'compliance_targets': context.get('compliance_targets', ['SOC 2', 'ISO 27001']) if context else ['SOC 2', 'ISO 27001']
            }
            
            # EJECUTAR LA HERRAMIENTA
            policy_content = self.policy_generator.generate_policy(
                detected_policy_type,
                company_context
            )
            
            policy_info = self.policy_generator.POLICY_TEMPLATES[detected_policy_type]
            
            response = f"""
## âš¡ EJECUTADO CON POLICY GENERATOR

He generado la polÃ­tica solicitada automÃ¡ticamente.

## ğŸ“Š RESULTADO

**PolÃ­tica Generada:** {policy_info['name']}
**Frameworks de Referencia:** {', '.join(policy_info['frameworks'])}
**Controles Aplicables:** {', '.join(policy_info['controls'])}
**Longitud:** ~{len(policy_content.split())} palabras

## ğŸ“„ CONTENIDO DE LA POLÃTICA

{policy_content}

## ğŸ§  MI RAZONAMIENTO

**Por quÃ© ejecutÃ© en MODO DIRECTO:**
1. âœ… Tarea clara: generar una polÃ­tica especÃ­fica ({detected_policy_type})
2. âœ… Bajo riesgo: solo generaciÃ³n de documentaciÃ³n
3. âœ… No requiere datos sensibles
4. âœ… Resultado predecible y seguro

**Herramienta utilizada:** Policy Generator
**Proceso:** DetectÃ© keywords â†’ IdentifiquÃ© tipo de polÃ­tica â†’ EjecutÃ© generaciÃ³n â†’ EntreguÃ© resultado completo

ğŸ’¡ **Nota:** Esta polÃ­tica requiere revisiÃ³n legal antes de implementaciÃ³n formal.
"""
            
            return {
                "mode": "DIRECT",
                "status": "executed",
                "tool_used": "policy_generator",
                "policy_type": detected_policy_type,
                "policy_content": policy_content,
                "results": response,
                "message": f"âš¡ MODO DIRECTO - Ejecutado con Policy Generator\n\n{response}",
                "requires_user_action": False
            }
        
        # SI NO ES GENERACIÃ“N DE POLÃTICA, FLUJO NORMAL CON LLM
        else:
            if self.use_llm and self.llm:
                prompt = f"""
Eres M.A.R.T.I.N. en MODO DIRECTO - agente autÃ³nomo.

Tu trabajo es:
1. Analizar y ejecutar inmediatamente
2. Reportar resultados
3. Explicar tu razonamiento

Tarea: {task}

Responde en este formato:

## âš¡ EJECUTADO
[QuÃ© acciones tomaste]

## ğŸ“Š RESULTADOS
[Resultados obtenidos]

## ğŸ§  MI RAZONAMIENTO
Por quÃ© lo hice asÃ­:
- [RazÃ³n 1]
- [RazÃ³n 2]
"""
                try:
                    response = self.llm.predict(prompt)
                except Exception as e:
                    response = f"Error al llamar LLM: {e}\n"
                    response += self._generate_direct_mock(task)
            else:
                response = self._generate_direct_mock(task)
            
            return {
                "mode": "DIRECT",
                "status": "executed",
                "results": response,
                "message": f"âš¡ MODO DIRECTO - Ejecutado automÃ¡ticamente\n\n{response}",
                "requires_user_action": False
            }
    
    def safe_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO SEGURO: Genera plan, AUTO-VALIDA, luego decide
        
        Comportamiento:
        1. Analiza la tarea
        2. Genera plan de acciÃ³n
        3. SE AUTO-CRITICA (validaciÃ³n de riesgos)
        4. Si pasa validaciÃ³n â†’ ejecuta con precauciones
        5. Si NO pasa â†’ sugiere alternativa segura
        """
        
        if self.use_llm and self.llm:
            # Paso 1: Generar plan
            plan_prompt = f"Genera un plan de acciÃ³n especÃ­fico para: {task}"
            try:
                plan = self.llm.predict(plan_prompt)
            except:
                plan = f"Plan para: {task}"
            
            # Paso 2: AUTO-VALIDACIÃ“N
            validation_prompt = f"""
Eres un validador de seguridad crÃ­tico.

Tarea: {task}
Plan: {plan}

Analiza riesgos:
1. Â¿Es destructivo?
2. Â¿Puede causar pÃ©rdida de datos?
3. Â¿Es reversible?

Responde:

NIVEL DE RIESGO: [BAJO/MEDIO/ALTO/CRÃTICO]

RIESGOS:
- [Riesgo 1]

DECISIÃ“N: [APROBAR/RECHAZAR]

SI RECHAZAS:
ALTERNATIVA: [alternativa segura]

SI APRUEBAS:
PRECAUCIONES: [lista]
"""
            try:
                validation = self.llm.predict(validation_prompt)
            except:
                validation = self._generate_safe_validation_mock(task)
        else:
            plan = f"Plan para: {task}"
            validation = self._generate_safe_validation_mock(task)
        
        # Analizar resultado
        if "RECHAZAR" in validation or "CRÃTICO" in validation or "ALTO" in validation:
            return {
                "mode": "SAFE",
                "status": "blocked",
                "validation_failed": True,
                "original_plan": plan,
                "validation_report": validation,
                "message": f"ğŸ›¡ï¸ MODO SEGURO - ACCIÃ“N BLOQUEADA\n\n{validation}",
                "requires_user_action": True
            }
        else:
            return {
                "mode": "SAFE",
                "status": "approved_and_executed",
                "validation_passed": True,
                "plan": plan,
                "validation_report": validation,
                "message": f"ğŸ›¡ï¸ MODO SEGURO - Validado y ejecutado\n\n{validation}\n\nâœ… EJECUTADO con precauciones.",
                "requires_user_action": False
            }
    
    # MÃ©todos de respuestas simuladas
    
    def _generate_passive_mock(self, task: str) -> str:
        return f"""
## ğŸ“‹ MI ANÃLISIS
He analizado tu solicitud: "{task}"

## ğŸ¯ PLAN PROPUESTO
1. Analizar requisitos especÃ­ficos (5 min)
2. Preparar documentaciÃ³n necesaria (15 min)
3. Ejecutar acciones principales (20 min)
4. Verificar resultados (10 min)

## âš ï¸ CONSIDERACIONES
- Requiere acceso a ciertos recursos
- Es importante revisar permisos necesarios

Â¿Procedo con este plan?
"""
    
    def _generate_direct_mock(self, task: str) -> str:
        return f"""
## âš¡ EJECUTADO
He completado la tarea: "{task}"

Acciones realizadas:
- AnalicÃ© los requisitos
- EjecutÃ© el proceso principal
- ValidÃ© los resultados

## ğŸ“Š RESULTADOS
âœ… Tarea completada exitosamente
ğŸ“ DocumentaciÃ³n generada
ğŸ” ValidaciÃ³n: OK

## ğŸ§  MI RAZONAMIENTO
Por quÃ© lo hice asÃ­:
- La tarea era clara y especÃ­fica
- No habÃ­a riesgos de seguridad
- EjecuciÃ³n directa mÃ¡s eficiente
"""
    
    def _generate_safe_validation_mock(self, task: str) -> str:
        danger_words = ['delete', 'remove', 'destroy', 'disable', 'drop', 'eliminar', 'borrar']
        is_dangerous = any(word in task.lower() for word in danger_words)
        
        if is_dangerous:
            return """
NIVEL DE RIESGO: ALTO

RIESGOS IDENTIFICADOS:
- AcciÃ³n potencialmente destructiva
- Puede causar pÃ©rdida de datos
- Afecta recursos crÃ­ticos
- DifÃ­cil de revertir

DECISIÃ“N: RECHAZAR

ALTERNATIVA SEGURA:
1. Crear backup completo primero
2. Ejecutar en ambiente de prueba
3. Implementar procedimiento de rollback
4. Obtener aprobaciÃ³n explÃ­cita
"""
        else:
            return """
NIVEL DE RIESGO: BAJO

RIESGOS IDENTIFICADOS:
- Riesgo mÃ­nimo detectado
- OperaciÃ³n de solo lectura
- No afecta datos crÃ­ticos

DECISIÃ“N: APROBAR

PRECAUCIONES:
- Logging habilitado
- Monitoreo activo
- ValidaciÃ³n post-ejecuciÃ³n
"""


# Test
if __name__ == "__main__":
    print("ğŸ§ª TESTING REASONING ENGINES CON TOOLS\n")
    
    engines = ReasoningEngines(use_llm=False)
    
    print("â”â”â” TEST 1: PASSIVE â”â”â”")
    result1 = engines.passive_reasoning("AyÃºdame con SOC 2")
    print(result1['message'][:200] + "...")
    print()
    
    print("â”â”â” TEST 2: DIRECT (sin tool) â”â”â”")
    result2 = engines.direct_reasoning("ExplÃ­came quÃ© es compliance")
    print(result2['message'][:200] + "...")
    print()
    
    print("â”â”â” TEST 3: DIRECT (CON POLICY GENERATOR) â”â”â”")
    result3 = engines.direct_reasoning("Genera polÃ­tica de contraseÃ±as")
    print(f"Tool usado: {result3.get('tool_used', 'ninguno')}")
    print(f"Tipo de polÃ­tica: {result3.get('policy_type', 'N/A')}")
    print(f"Longitud de polÃ­tica: {len(result3.get('policy_content', ''))} caracteres")
    print()
    
    print("â”â”â” TEST 4: SAFE â”â”â”")
    result4 = engines.safe_reasoning("Delete all users")
    print(result4['message'][:200] + "...")
    print()
    
    print("âœ… Tests completados!")