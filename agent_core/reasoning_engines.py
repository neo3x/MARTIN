"""
Los 3 motores de razonamiento de M.A.R.T.I.N.
Soporta OpenAI (GPT-4) y Anthropic (Claude)
"""
from typing import Dict, Any
import os

class ReasoningEngines:
    """
    Contiene los 3 modos de razonamiento de M.A.R.T.I.N.
    Soporta mÃºltiples LLMs: OpenAI y Claude
    """
    
    def __init__(self, use_llm: bool = False, llm_provider: str = "auto"):
        """
        Args:
            use_llm: Si True, usa LLM real. Si False, usa respuestas simuladas.
            llm_provider: "openai", "claude", o "auto" (detecta automÃ¡ticamente)
        """
        self.use_llm = use_llm
        self.llm = None
        self.llm_provider = None
        
        if self.use_llm:
            self.llm_provider = self._initialize_llm(llm_provider)
            if not self.llm:
                print("âš ï¸ No se pudo inicializar LLM. Usando modo simulado.")
                self.use_llm = False
    
    def _initialize_llm(self, provider: str):
        """Inicializa el LLM segÃºn el proveedor especificado"""
        
        # Auto-detectar quÃ© API key estÃ¡ disponible
        if provider == "auto":
            if os.getenv("ANTHROPIC_API_KEY"):
                provider = "claude"
                print("ğŸ” Auto-detectado: Claude API key disponible")
            elif os.getenv("OPENAI_API_KEY"):
                provider = "openai"
                print("ğŸ” Auto-detectado: OpenAI API key disponible")
            else:
                print("âš ï¸ No se encontrÃ³ OPENAI_API_KEY ni ANTHROPIC_API_KEY")
                return None
        
        # Inicializar OpenAI
        if provider == "openai":
            try:
                from langchain.chat_models import ChatOpenAI
                api_key = os.getenv("OPENAI_API_KEY")
                
                if not api_key:
                    print("âš ï¸ OPENAI_API_KEY no configurada")
                    return None
                
                self.llm = ChatOpenAI(
                    model="gpt-4",
                    temperature=0,
                    api_key=api_key
                )
                print("âœ… LLM inicializado: OpenAI GPT-4")
                return "openai"
                
            except ImportError:
                print("âš ï¸ langchain no instalado")
                return None
            except Exception as e:
                print(f"âš ï¸ Error inicializando OpenAI: {e}")
                return None
        
        # Inicializar Claude
        elif provider == "claude":
            try:
                from langchain.chat_models import ChatAnthropic
                api_key = os.getenv("ANTHROPIC_API_KEY")
                
                if not api_key:
                    print("âš ï¸ ANTHROPIC_API_KEY no configurada")
                    return None
                
                self.llm = ChatAnthropic(
                    model="claude-3-5-sonnet-20241022",
                    temperature=0,
                    anthropic_api_key=api_key
                )
                print("âœ… LLM inicializado: Anthropic Claude 3.5 Sonnet")
                return "claude"
                
            except ImportError:
                print("âš ï¸ anthropic no instalado. Instala con: pip install anthropic")
                return None
            except Exception as e:
                print(f"âš ï¸ Error inicializando Claude: {e}")
                return None
        
        else:
            print(f"âš ï¸ Proveedor desconocido: {provider}")
            return None
    
    def passive_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO PASIVO: Genera plan pero NO ejecuta
        
        Comportamiento:
        1. Analiza la tarea
        2. Genera un plan detallado
        3. Explica quÃ© harÃ¡
        4. ESPERA confirmaciÃ³n del usuario
        """
        
        if self.use_llm and self.llm:
            prompt = f"""
Eres M.A.R.T.I.N., un agente de IA en MODO PASIVO.

Tu trabajo es:
1. Analizar la tarea del usuario
2. Proponer un plan estructurado
3. Explicar quÃ© harÃ¡s
4. NO ejecutar nada hasta recibir confirmaciÃ³n

Tarea: {task}
Contexto: {context if context else "No hay contexto adicional"}

Responde en este formato:

## ğŸ“‹ MI ANÃLISIS
[CÃ³mo entiendes la tarea]

## ğŸ¯ PLAN PROPUESTO
1. [Paso 1] (tiempo estimado)
2. [Paso 2] (tiempo estimado)

## âš ï¸ CONSIDERACIONES
- [Punto importante 1]
- [Punto importante 2]

Â¿Procedo con este plan?
"""
            try:
                response = self.llm.predict(prompt)
            except Exception as e:
                response = f"Error al llamar LLM: {e}\n"
                response += self._generate_passive_mock(task)
        else:
            response = self._generate_passive_mock(task)
        
        return {
            "mode": "PASSIVE",
            "status": "awaiting_confirmation",
            "plan": response,
            "message": f"ğŸ“‹ MODO PASIVO ACTIVADO\n\n{response}",
            "requires_user_action": True
        }
    
    def direct_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO DIRECTO: Genera plan Y ejecuta automÃ¡ticamente
        
        Comportamiento:
        1. Analiza la tarea
        2. Genera plan de acciÃ³n
        3. EJECUTA sin preguntar
        4. Reporta resultados
        5. Explica su razonamiento
        """
        
        if self.use_llm and self.llm:
            prompt = f"""
Eres M.A.R.T.I.N. en MODO DIRECTO - agente autÃ³nomo.

Tu trabajo es:
1. Analizar y ejecutar inmediatamente
2. Reportar resultados
3. Explicar tu razonamiento

Tarea: {task}

Responde en este formato:

## âš¡ EJECUTADO
[QuÃ© acciones tomaste]

## ğŸ“Š RESULTADOS
[Resultados obtenidos]

## ğŸ§  MI RAZONAMIENTO
Por quÃ© lo hice asÃ­:
- [RazÃ³n 1]
- [RazÃ³n 2]
"""
            try:
                response = self.llm.predict(prompt)
            except Exception as e:
                response = f"Error al llamar LLM: {e}\n"
                response += self._generate_direct_mock(task)
        else:
            response = self._generate_direct_mock(task)
        
        return {
            "mode": "DIRECT",
            "status": "executed",
            "results": response,
            "message": f"âš¡ MODO DIRECTO - Ejecutado automÃ¡ticamente\n\n{response}",
            "requires_user_action": False
        }
    
    def safe_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO SEGURO: Genera plan, AUTO-VALIDA, luego decide
        
        Comportamiento:
        1. Analiza la tarea
        2. Genera plan de acciÃ³n
        3. SE AUTO-CRITICA (validaciÃ³n de riesgos)
        4. Si pasa validaciÃ³n â†’ ejecuta con precauciones
        5. Si NO pasa â†’ sugiere alternativa segura
        """
        
        if self.use_llm and self.llm:
            # Paso 1: Generar plan
            plan_prompt = f"Genera un plan de acciÃ³n especÃ­fico para: {task}"
            try:
                plan = self.llm.predict(plan_prompt)
            except:
                plan = f"Plan para: {task}"
            
            # Paso 2: AUTO-VALIDACIÃ“N
            validation_prompt = f"""
Eres un validador de seguridad crÃ­tico.

Tarea: {task}
Plan: {plan}

Analiza riesgos:
1. Â¿Es destructivo?
2. Â¿Puede causar pÃ©rdida de datos?
3. Â¿Es reversible?

Responde:

NIVEL DE RIESGO: [BAJO/MEDIO/ALTO/CRÃTICO]

RIESGOS:
- [Riesgo 1]

DECISIÃ“N: [APROBAR/RECHAZAR]

SI RECHAZAS:
ALTERNATIVA: [alternativa segura]

SI APRUEBAS:
PRECAUCIONES: [lista]
"""
            try:
                validation = self.llm.predict(validation_prompt)
            except:
                validation = self._generate_safe_validation_mock(task)
        else:
            plan = f"Plan para: {task}"
            validation = self._generate_safe_validation_mock(task)
        
        # Analizar resultado
        if "RECHAZAR" in validation or "CRÃTICO" in validation or "ALTO" in validation:
            return {
                "mode": "SAFE",
                "status": "blocked",
                "validation_failed": True,
                "original_plan": plan,
                "validation_report": validation,
                "message": f"ğŸ›¡ï¸ MODO SEGURO - ACCIÃ“N BLOQUEADA\n\n{validation}",
                "requires_user_action": True
            }
        else:
            return {
                "mode": "SAFE",
                "status": "approved_and_executed",
                "validation_passed": True,
                "plan": plan,
                "validation_report": validation,
                "message": f"ğŸ›¡ï¸ MODO SEGURO - Validado y ejecutado\n\n{validation}\n\nâœ… EJECUTADO con precauciones.",
                "requires_user_action": False
            }
    
    # MÃ©todos de respuestas simuladas
    
    def _generate_passive_mock(self, task: str) -> str:
        return f"""
## ğŸ“‹ MI ANÃLISIS
He analizado tu solicitud: "{task}"

## ğŸ¯ PLAN PROPUESTO
1. Analizar requisitos especÃ­ficos (5 min)
2. Preparar documentaciÃ³n necesaria (15 min)
3. Ejecutar acciones principales (20 min)
4. Verificar resultados (10 min)

## âš ï¸ CONSIDERACIONES
- Requiere acceso a ciertos recursos
- Es importante revisar permisos necesarios

Â¿Procedo con este plan?
"""
    
    def _generate_direct_mock(self, task: str) -> str:
        return f"""
## âš¡ EJECUTADO
He completado la tarea: "{task}"

Acciones realizadas:
- AnalicÃ© los requisitos
- EjecutÃ© el proceso principal
- ValidÃ© los resultados

## ğŸ“Š RESULTADOS
âœ… Tarea completada exitosamente
ğŸ“ DocumentaciÃ³n generada
ğŸ” ValidaciÃ³n: OK

## ğŸ§  MI RAZONAMIENTO
Por quÃ© lo hice asÃ­:
- La tarea era clara y especÃ­fica
- No habÃ­a riesgos de seguridad
- EjecuciÃ³n directa mÃ¡s eficiente
"""
    
    def _generate_safe_validation_mock(self, task: str) -> str:
        danger_words = ['delete', 'remove', 'destroy', 'disable', 'drop', 'eliminar', 'borrar']
        is_dangerous = any(word in task.lower() for word in danger_words)
        
        if is_dangerous:
            return """
NIVEL DE RIESGO: ALTO

RIESGOS IDENTIFICADOS:
- AcciÃ³n potencialmente destructiva
- Puede causar pÃ©rdida de datos
- Afecta recursos crÃ­ticos
- DifÃ­cil de revertir

DECISIÃ“N: RECHAZAR

ALTERNATIVA SEGURA:
1. Crear backup completo primero
2. Ejecutar en ambiente de prueba
3. Implementar procedimiento de rollback
4. Obtener aprobaciÃ³n explÃ­cita
"""
        else:
            return """
NIVEL DE RIESGO: BAJO

RIESGOS IDENTIFICADOS:
- Riesgo mÃ­nimo detectado
- OperaciÃ³n de solo lectura
- No afecta datos crÃ­ticos

DECISIÃ“N: APROBAR

PRECAUCIONES:
- Logging habilitado
- Monitoreo activo
- ValidaciÃ³n post-ejecuciÃ³n
"""


# Test
if __name__ == "__main__":
    print("ğŸ§ª TESTING REASONING ENGINES\n")
    
    engines = ReasoningEngines(use_llm=False)
    
    print("â”â”â” TEST PASSIVE â”â”â”")
    result1 = engines.passive_reasoning("AyÃºdame con SOC 2")
    print(result1['message'])
    print()
    
    print("â”â”â” TEST DIRECT â”â”â”")
    result2 = engines.direct_reasoning("Genera polÃ­tica de passwords")
    print(result2['message'])
    print()
    
    print("â”â”â” TEST SAFE â”â”â”")
    result3 = engines.safe_reasoning("Delete all users")
    print(result3['message'])
    print()
    
    print("âœ… Tests completados!")