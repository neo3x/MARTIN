"""
Los 3 motores de razonamiento de M.A.R.T.I.N.
Soporta OpenAI (GPT-4) y Anthropic (Claude)
CON INTEGRACI√ìN DE TOOLS
"""
from typing import Dict, Any
import os
import sys
from pathlib import Path

# Importar tools
sys.path.insert(0, str(Path(__file__).parent.parent))
try:
    from tools.policy_generator import PolicyGenerator
except ImportError:
    PolicyGenerator = None
    print("‚ö†Ô∏è PolicyGenerator no disponible - instala dependencias")

class ReasoningEngines:
    """
    Contiene los 3 modos de razonamiento de M.A.R.T.I.N.
    Soporta m√∫ltiples LLMs: OpenAI y Claude
    AHORA CON HERRAMIENTAS REALES
    """
    
    def __init__(self, use_llm: bool = False, llm_provider: str = "auto"):
        """
        Args:
            use_llm: Si True, usa LLM real. Si False, usa respuestas simuladas.
            llm_provider: "openai", "claude", o "auto" (detecta autom√°ticamente)
        """
        self.use_llm = use_llm
        self.llm = None
        self.llm_provider = None
        
        if self.use_llm:
            self.llm_provider = self._initialize_llm(llm_provider)
            if not self.llm:
                print("‚ö†Ô∏è No se pudo inicializar LLM. Usando modo simulado.")
                self.use_llm = False
        
        # Inicializar herramientas
        if PolicyGenerator:
            self.policy_generator = PolicyGenerator(llm=self.llm if self.use_llm else None)
        else:
            self.policy_generator = None
    
    def _initialize_llm(self, provider: str):
        """Inicializa el LLM seg√∫n el proveedor especificado"""
        
        # Auto-detectar qu√© API key est√° disponible
        if provider == "auto":
            if os.getenv("ANTHROPIC_API_KEY"):
                provider = "claude"
                print("üîç Auto-detectado: Claude API key disponible")
            elif os.getenv("OPENAI_API_KEY"):
                provider = "openai"
                print("üîç Auto-detectado: OpenAI API key disponible")
            else:
                print("‚ö†Ô∏è No se encontr√≥ OPENAI_API_KEY ni ANTHROPIC_API_KEY")
                return None
        
        # Inicializar OpenAI
        if provider == "openai":
            try:
                from langchain.chat_models import ChatOpenAI
                api_key = os.getenv("OPENAI_API_KEY")
                
                if not api_key:
                    print("‚ö†Ô∏è OPENAI_API_KEY no configurada")
                    return None
                
                self.llm = ChatOpenAI(
                    model="gpt-4",
                    temperature=0,
                    api_key=api_key
                )
                print("‚úÖ LLM inicializado: OpenAI GPT-4")
                return "openai"
                
            except ImportError:
                print("‚ö†Ô∏è langchain no instalado")
                return None
            except Exception as e:
                print(f"‚ö†Ô∏è Error inicializando OpenAI: {e}")
                return None
        
        # Inicializar Claude
        elif provider == "claude":
            try:
                from langchain.chat_models import ChatAnthropic
                api_key = os.getenv("ANTHROPIC_API_KEY")
                
                if not api_key:
                    print("‚ö†Ô∏è ANTHROPIC_API_KEY no configurada")
                    return None
                
                self.llm = ChatAnthropic(
                    model="claude-3-5-sonnet-20241022",
                    temperature=0,
                    anthropic_api_key=api_key
                )
                print("‚úÖ LLM inicializado: Anthropic Claude 3.5 Sonnet")
                return "claude"
                
            except ImportError:
                print("‚ö†Ô∏è anthropic no instalado. Instala con: pip install anthropic")
                return None
            except Exception as e:
                print(f"‚ö†Ô∏è Error inicializando Claude: {e}")
                return None
        
        else:
            print(f"‚ö†Ô∏è Proveedor desconocido: {provider}")
            return None
    
    def passive_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO PASIVO: Genera plan pero NO ejecuta
        
        Comportamiento:
        1. Analiza la tarea
        2. Genera un plan detallado
        3. Explica qu√© har√°
        4. ESPERA confirmaci√≥n del usuario
        """
        
        if self.use_llm and self.llm:
            prompt = f"""
Eres M.A.R.T.I.N., un agente de IA en MODO PASIVO.

Tu trabajo es:
1. Analizar la tarea del usuario
2. Proponer un plan estructurado
3. Explicar qu√© har√°s
4. NO ejecutar nada hasta recibir confirmaci√≥n

Tarea: {task}
Contexto: {context if context else "No hay contexto adicional"}

Responde en este formato:

## üìã MI AN√ÅLISIS
[C√≥mo entiendes la tarea]

## üéØ PLAN PROPUESTO
1. [Paso 1] (tiempo estimado)
2. [Paso 2] (tiempo estimado)

## ‚ö†Ô∏è CONSIDERACIONES
- [Punto importante 1]
- [Punto importante 2]

¬øProcedo con este plan?
"""
            try:
                response = self.llm.predict(prompt)
            except Exception as e:
                response = f"Error al llamar LLM: {e}\n"
                response += self._generate_passive_mock(task)
        else:
            response = self._generate_passive_mock(task)
        
        return {
            "mode": "PASSIVE",
            "status": "awaiting_confirmation",
            "plan": response,
            "message": f"üìã MODO PASIVO ACTIVADO\n\n{response}",
            "requires_user_action": True
        }
    
    def direct_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO DIRECTO: Genera plan Y ejecuta autom√°ticamente
        
        AHORA CON DETECCI√ìN Y EJECUCI√ìN DE HERRAMIENTAS
        """
        
        task_lower = task.lower()
        
        # DETECTAR SI DEBE USAR POLICY GENERATOR
        policy_keywords = ['genera', 'crea', 'escribe', 'crear', 'generar', 'policy', 'pol√≠tica', 'politica']
        policy_types = {
            'password': 'password_policy',
            'contrase√±a': 'password_policy',
            'contrase√±as': 'password_policy',
            'incidente': 'incident_response',
            'incidentes': 'incident_response',
            'incident': 'incident_response',
            'acceso': 'access_control',
            'access': 'access_control',
            'dato': 'data_classification',
            'datos': 'data_classification',
            'data': 'data_classification',
            'backup': 'backup_recovery',
            'recuperaci√≥n': 'backup_recovery',
            'recuperacion': 'backup_recovery'
        }
        
        should_generate_policy = any(keyword in task_lower for keyword in policy_keywords)
        detected_policy_type = None
        
        if should_generate_policy:
            for keyword, policy_type in policy_types.items():
                if keyword in task_lower:
                    detected_policy_type = policy_type
                    break
        
        # SI DEBE GENERAR POL√çTICA Y TENEMOS LA TOOL
        if should_generate_policy and detected_policy_type and self.policy_generator:
            
            # Contexto de la empresa
            company_context = {
                'name': context.get('company_name', 'La Organizaci√≥n') if context else 'La Organizaci√≥n',
                'size': context.get('company_size', '20-50') if context else '20-50',
                'industry': context.get('industry', 'Tecnolog√≠a / SaaS') if context else 'Tecnolog√≠a / SaaS',
                'tech_stack': context.get('tech_stack', 'Cloud-based') if context else 'Cloud-based',
                'compliance_targets': context.get('compliance_targets', ['SOC 2', 'ISO 27001']) if context else ['SOC 2', 'ISO 27001']
            }
            
            # EJECUTAR LA HERRAMIENTA
            policy_content = self.policy_generator.generate_policy(
                detected_policy_type,
                company_context
            )
            
            policy_info = self.policy_generator.POLICY_TEMPLATES[detected_policy_type]
            
            response = f"""
## ‚ö° EJECUTADO CON POLICY GENERATOR

He generado la pol√≠tica solicitada autom√°ticamente.

## üìä RESULTADO

**Pol√≠tica Generada:** {policy_info['name']}
**Frameworks de Referencia:** {', '.join(policy_info['frameworks'])}
**Controles Aplicables:** {', '.join(policy_info['controls'])}
**Longitud:** ~{len(policy_content.split())} palabras

## üìÑ CONTENIDO DE LA POL√çTICA

{policy_content}

## üß† MI RAZONAMIENTO

**Por qu√© ejecut√© en MODO DIRECTO:**
1. ‚úÖ Tarea clara: generar una pol√≠tica espec√≠fica ({detected_policy_type})
2. ‚úÖ Bajo riesgo: solo generaci√≥n de documentaci√≥n
3. ‚úÖ No requiere datos sensibles
4. ‚úÖ Resultado predecible y seguro

**Herramienta utilizada:** Policy Generator
**Proceso:** Detect√© keywords ‚Üí Identifiqu√© tipo de pol√≠tica ‚Üí Ejecut√© generaci√≥n ‚Üí Entregu√© resultado completo

üí° **Nota:** Esta pol√≠tica requiere revisi√≥n legal antes de implementaci√≥n formal.
"""
            
            return {
                "mode": "DIRECT",
                "status": "executed",
                "tool_used": "policy_generator",
                "policy_type": detected_policy_type,
                "policy_content": policy_content,
                "results": response,
                "message": f"‚ö° MODO DIRECTO - Ejecutado con Policy Generator\n\n{response}",
                "requires_user_action": False
            }
        
        # SI NO ES GENERACI√ìN DE POL√çTICA, FLUJO NORMAL CON LLM
        else:
            if self.use_llm and self.llm:
                prompt = f"""
Eres M.A.R.T.I.N. en MODO DIRECTO - agente aut√≥nomo.

Tu trabajo es:
1. Analizar y ejecutar inmediatamente
2. Reportar resultados
3. Explicar tu razonamiento

Tarea: {task}

Responde en este formato:

## ‚ö° EJECUTADO
[Qu√© acciones tomaste]

## üìä RESULTADOS
[Resultados obtenidos]

## üß† MI RAZONAMIENTO
Por qu√© lo hice as√≠:
- [Raz√≥n 1]
- [Raz√≥n 2]
"""
                try:
                    response = self.llm.predict(prompt)
                except Exception as e:
                    response = f"Error al llamar LLM: {e}\n"
                    response += self._generate_direct_mock(task)
            else:
                response = self._generate_direct_mock(task)
            
            return {
                "mode": "DIRECT",
                "status": "executed",
                "results": response,
                "message": f"‚ö° MODO DIRECTO - Ejecutado autom√°ticamente\n\n{response}",
                "requires_user_action": False
            }
    
    def safe_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO SEGURO: Genera plan, AUTO-VALIDA, luego decide
        
        Comportamiento:
        1. Analiza la tarea
        2. Genera plan de acci√≥n
        3. SE AUTO-CRITICA (validaci√≥n de riesgos)
        4. Si pasa validaci√≥n ‚Üí ejecuta con precauciones
        5. Si NO pasa ‚Üí sugiere alternativa segura
        """
        
        if self.use_llm and self.llm:
            # Paso 1: Generar plan
            plan_prompt = f"Genera un plan de acci√≥n espec√≠fico para: {task}"
            try:
                plan = self.llm.predict(plan_prompt)
            except:
                plan = f"Plan para: {task}"
            
            # Paso 2: AUTO-VALIDACI√ìN
            validation_prompt = f"""
Eres un validador de seguridad cr√≠tico.

Tarea: {task}
Plan: {plan}

Analiza riesgos:
1. ¬øEs destructivo?
2. ¬øPuede causar p√©rdida de datos?
3. ¬øEs reversible?

Responde:

NIVEL DE RIESGO: [BAJO/MEDIO/ALTO/CR√çTICO]

RIESGOS:
- [Riesgo 1]

DECISI√ìN: [APROBAR/RECHAZAR]

SI RECHAZAS:
ALTERNATIVA: [alternativa segura]

SI APRUEBAS:
PRECAUCIONES: [lista]
"""
            try:
                validation = self.llm.predict(validation_prompt)
            except:
                validation = self._generate_safe_validation_mock(task)
        else:
            plan = f"Plan para: {task}"
            validation = self._generate_safe_validation_mock(task)
        
        # Analizar resultado
        if "RECHAZAR" in validation or "CR√çTICO" in validation or "ALTO" in validation:
            return {
                "mode": "SAFE",
                "status": "blocked",
                "validation_failed": True,
                "original_plan": plan,
                "validation_report": validation,
                "message": f"üõ°Ô∏è MODO SEGURO - ACCI√ìN BLOQUEADA\n\n{validation}",
                "requires_user_action": True
            }
        else:
            return {
                "mode": "SAFE",
                "status": "approved_and_executed",
                "validation_passed": True,
                "plan": plan,
                "validation_report": validation,
                "message": f"üõ°Ô∏è MODO SEGURO - Validado y ejecutado\n\n{validation}\n\n‚úÖ EJECUTADO con precauciones.",
                "requires_user_action": False
            }
    
    # M√©todos de respuestas simuladas
    
    def _generate_passive_mock(self, task: str) -> str:
        return f"""
## üìã MI AN√ÅLISIS
He analizado tu solicitud: "{task}"

## üéØ PLAN PROPUESTO
1. Analizar requisitos espec√≠ficos (5 min)
2. Preparar documentaci√≥n necesaria (15 min)
3. Ejecutar acciones principales (20 min)
4. Verificar resultados (10 min)

## ‚ö†Ô∏è CONSIDERACIONES
- Requiere acceso a ciertos recursos
- Es importante revisar permisos necesarios

¬øProcedo con este plan?
"""
    
    def _generate_direct_mock(self, task: str) -> str:
        return f"""
## ‚ö° EJECUTADO
He completado la tarea: "{task}"

Acciones realizadas:
- Analic√© los requisitos
- Ejecut√© el proceso principal
- Valid√© los resultados

## üìä RESULTADOS
‚úÖ Tarea completada exitosamente
üìÅ Documentaci√≥n generada
üîç Validaci√≥n: OK

## üß† MI RAZONAMIENTO
Por qu√© lo hice as√≠:
- La tarea era clara y espec√≠fica
- No hab√≠a riesgos de seguridad
- Ejecuci√≥n directa m√°s eficiente
"""
    
    def _generate_safe_validation_mock(self, task: str) -> str:
        danger_words = ['delete', 'remove', 'destroy', 'disable', 'drop', 'eliminar', 'borrar']
        is_dangerous = any(word in task.lower() for word in danger_words)
        
        if is_dangerous:
            return """
NIVEL DE RIESGO: ALTO

RIESGOS IDENTIFICADOS:
- Acci√≥n potencialmente destructiva
- Puede causar p√©rdida de datos
- Afecta recursos cr√≠ticos
- Dif√≠cil de revertir

DECISI√ìN: RECHAZAR

ALTERNATIVA SEGURA:
1. Crear backup completo primero
2. Ejecutar en ambiente de prueba
3. Implementar procedimiento de rollback
4. Obtener aprobaci√≥n expl√≠cita
"""
        else:
            return """
NIVEL DE RIESGO: BAJO

RIESGOS IDENTIFICADOS:
- Riesgo m√≠nimo detectado
- Operaci√≥n de solo lectura
- No afecta datos cr√≠ticos

DECISI√ìN: APROBAR

PRECAUCIONES:
- Logging habilitado
- Monitoreo activo
- Validaci√≥n post-ejecuci√≥n
"""


# Test
if __name__ == "__main__":
    print("üß™ TESTING REASONING ENGINES CON TOOLS\n")
    
    engines = ReasoningEngines(use_llm=False)
    
    print("‚îÅ‚îÅ‚îÅ TEST 1: PASSIVE ‚îÅ‚îÅ‚îÅ")
    result1 = engines.passive_reasoning("Ay√∫dame con SOC 2")
    print(result1['message'][:200] + "...")
    print()
    
    print("‚îÅ‚îÅ‚îÅ TEST 2: DIRECT (sin tool) ‚îÅ‚îÅ‚îÅ")
    result2 = engines.direct_reasoning("Expl√≠came qu√© es compliance")
    print(result2['message'][:200] + "...")
    print()
    
    print("‚îÅ‚îÅ‚îÅ TEST 3: DIRECT (CON POLICY GENERATOR) ‚îÅ‚îÅ‚îÅ")
    result3 = engines.direct_reasoning("Genera pol√≠tica de contrase√±as")
    print(f"Tool usado: {result3.get('tool_used', 'ninguno')}")
    print(f"Tipo de pol√≠tica: {result3.get('policy_type', 'N/A')}")
    print(f"Longitud de pol√≠tica: {len(result3.get('policy_content', ''))} caracteres")
    print()
    
    print("‚îÅ‚îÅ‚îÅ TEST 4: SAFE ‚îÅ‚îÅ‚îÅ")
    result4 = engines.safe_reasoning("Delete all users")
    print(result4['message'][:200] + "...")
    print()
    
    print("‚úÖ Tests completados!")