# ğŸ§  M.A.R.T.I.N. - Roadmap Completo y Detallado

## ğŸ¯ La Propuesta (Recordatorio)

**M.A.R.T.I.N. = Motor de Razonamiento Adaptativo aplicado a Compliance**

### Los 3 Modos de Razonamiento:
- ğŸŸ¦ **MODO PASIVO:** Pregunta antes de actuar (tareas ambiguas, exploraciÃ³n)
- ğŸŸ© **MODO DIRECTO:** ActÃºa autÃ³nomamente (tareas claras, bajo riesgo)
- ğŸŸ¨ **MODO SEGURO:** Auto-valida antes de actuar (alto riesgo, producciÃ³n)

### Caso de Uso:
Compliance automation para startups (SOC 2, ISO 27001)

### La InnovaciÃ³n:
No es "quÃ© hace" sino "cÃ³mo razona" â†’ Razonamiento tri-modal adaptativo

---

## â° CONTEXTO TEMPORAL

### Fechas Clave:
- **HOY:** Domingo 19 Octubre, 23:00 hrs
- **Hackathon inicia:** SÃ¡bado 25 Octubre, 09:00 hrs
- **Hackathon termina:** Domingo 26 Octubre, 18:00 hrs
- **Tiempo de preparaciÃ³n:** 5 dÃ­as y medio (Lun 20 â†’ Vie 24)

---

# ğŸ“† ROADMAP DETALLADO DE PREPARACIÃ“N

## ğŸŒ™ Domingo 19 Octubre (23:00) - HOY

### AcciÃ³n inmediata:
- âœ… **Revisar y entender este roadmap completo** (15 min)
- âœ… **Dormir bien** (7-8 horas mÃ­nimo)
- âœ… **Mentalidad:** MaÃ±ana arrancamos con todo ğŸš€

---

## ğŸ“… Lunes 20 Octubre - DÃA 1

### Objetivo del dÃ­a:
**Fundamentos tÃ©cnicos + ModeSelector funcional**

---

### MAÃ‘ANA (09:00 - 13:00) - 4 horas

#### 09:00 - 10:00 | Setup del entorno (1 hora)

**Tareas:**
```bash
# 1. Crear estructura del proyecto
mkdir martin-agent
cd martin-agent
python -m venv venv

# Windows:
venv\Scripts\activate

# Mac/Linux:
source venv/bin/activate

# 2. Instalar dependencias
pip install langchain==0.1.0
pip install openai==1.0.0
pip install anthropic==0.8.0  # Alternativa a OpenAI
pip install python-dotenv==1.0.0
pip install gradio==4.0.0
pip install requests==2.31.0
pip install beautifulsoup4==4.12.0

# 3. Crear estructura de carpetas
mkdir -p {agents,tools,tests,docs}
touch .env README.md requirements.txt

# 4. Configurar .env
echo "OPENAI_API_KEY=tu-key-aqui" > .env
echo "GITHUB_TOKEN=tu-token-aqui" >> .env
```

**API Keys necesarias:**
- [ ] OpenAI API Key: https://platform.openai.com/api-keys
- [ ] GitHub Personal Token: https://github.com/settings/tokens (scope: read:org, repo)

**Verificar instalaciÃ³n:**
```python
# test_setup.py
from langchain.chat_models import ChatOpenAI
import os
from dotenv import load_dotenv

load_dotenv()

llm = ChatOpenAI(model="gpt-4", temperature=0)
response = llm.predict("Di 'Setup exitoso'")
print(response)
```

---

#### 10:00 - 11:30 | ModeSelector v1 (1.5 horas)

**Archivo:** `agents/mode_selector.py`

```python
"""
ModeSelector - El cerebro que decide cÃ³mo M.A.R.T.I.N. debe razonar
"""
from typing import Dict, Literal
import re

ModeType = Literal["PASSIVE", "DIRECT", "SAFE"]

class ModeSelector:
    """
    Analiza la tarea y contexto para determinar el modo de razonamiento Ã³ptimo.
    """
    
    # Palabras clave que indican alto riesgo
    DANGER_KEYWORDS = [
        'delete', 'remove', 'destroy', 'drop', 'disable', 
        'terminate', 'kill', 'shutdown', 'revoke', 'block'
    ]
    
    # Palabras que indican ambigÃ¼edad o necesidad de clarificaciÃ³n
    VAGUE_KEYWORDS = [
        'ayuda', 'ayÃºdame', 'help', 'cÃ³mo', 'quÃ© debo', 'no sÃ©'
    ]
    
    def __init__(self):
        self.decision_log = []
    
    def select_mode(self, task: str, context: Dict = None) -> ModeType:
        """
        Decide el modo de razonamiento basado en anÃ¡lisis de la tarea.
        
        Args:
            task: InstrucciÃ³n del usuario
            context: InformaciÃ³n contextual (environment, user_role, etc.)
        
        Returns:
            Modo seleccionado: "PASSIVE", "DIRECT", o "SAFE"
        """
        if context is None:
            context = {}
        
        # AnÃ¡lisis de factores
        risk_score = self._assess_risk(task, context)
        clarity_score = self._assess_clarity(task)
        environment = context.get('environment', 'development')
        
        # Logging de decisiÃ³n
        decision_factors = {
            'task': task[:50] + '...' if len(task) > 50 else task,
            'risk_score': risk_score,
            'clarity_score': clarity_score,
            'environment': environment
        }
        
        # Reglas de decisiÃ³n (orden de prioridad)
        
        # 1. ProducciÃ³n siempre va a SAFE
        if environment == 'production':
            mode = "SAFE"
            reason = "Entorno de producciÃ³n detectado"
        
        # 2. Alto riesgo siempre va a SAFE
        elif risk_score >= 0.7:
            mode = "SAFE"
            reason = f"Riesgo alto detectado (score: {risk_score})"
        
        # 3. Baja claridad va a PASSIVE
        elif clarity_score < 0.5:
            mode = "PASSIVE"
            reason = f"Tarea ambigua o requiere clarificaciÃ³n (clarity: {clarity_score})"
        
        # 4. Clara y segura va a DIRECT
        else:
            mode = "DIRECT"
            reason = "Tarea clara y de bajo riesgo"
        
        # Guardar log de decisiÃ³n
        decision_factors['selected_mode'] = mode
        decision_factors['reason'] = reason
        self.decision_log.append(decision_factors)
        
        return mode
    
    def _assess_risk(self, task: str, context: Dict) -> float:
        """
        Calcula score de riesgo (0.0 - 1.0)
        
        Factores:
        - Palabras peligrosas en la tarea
        - Recursos crÃ­ticos mencionados
        - Scope de impacto
        """
        risk = 0.0
        task_lower = task.lower()
        
        # Factor 1: Palabras peligrosas
        danger_words_found = sum(1 for word in self.DANGER_KEYWORDS if word in task_lower)
        if danger_words_found > 0:
            risk += 0.4 * min(danger_words_found / 2, 1.0)
        
        # Factor 2: Recursos crÃ­ticos
        critical_resources = ['database', 'db', 'producciÃ³n', 'production', 
                            'payment', 'billing', 'auth', 'users', 'admin']
        if any(resource in task_lower for resource in critical_resources):
            risk += 0.3
        
        # Factor 3: Scope amplio
        broad_scope_indicators = ['all', 'every', 'todos', 'cada', 'entire', 'completo']
        if any(indicator in task_lower for indicator in broad_scope_indicators):
            risk += 0.2
        
        # Factor 4: Contexto de ambiente
        if context.get('has_active_users', False):
            risk += 0.1
        
        return min(risk, 1.0)
    
    def _assess_clarity(self, task: str) -> float:
        """
        Calcula score de claridad (0.0 - 1.0)
        
        Factores:
        - Presencia de preguntas
        - Longitud de la instrucciÃ³n
        - Palabras vagas
        - Especificidad
        """
        clarity = 1.0
        task_lower = task.lower()
        
        # Factor 1: Es una pregunta
        if '?' in task:
            clarity -= 0.3
        
        # Factor 2: Palabras vagas
        vague_words_found = sum(1 for word in self.VAGUE_KEYWORDS if word in task_lower)
        clarity -= 0.15 * vague_words_found
        
        # Factor 3: Longitud (muy corto = vago)
        word_count = len(task.split())
        if word_count < 5:
            clarity -= 0.3
        elif word_count < 3:
            clarity -= 0.5
        
        # Factor 4: No menciona recursos especÃ­ficos
        if not any(char.isupper() for char in task):  # Sin nombres propios/especÃ­ficos
            clarity -= 0.2
        
        return max(clarity, 0.0)
    
    def explain_last_decision(self) -> str:
        """Retorna explicaciÃ³n de la Ãºltima decisiÃ³n tomada"""
        if not self.decision_log:
            return "No hay decisiones registradas aÃºn"
        
        last = self.decision_log[-1]
        return f"""
DecisiÃ³n del ModeSelector:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Tarea: "{last['task']}"
Modo seleccionado: {last['selected_mode']}
RazÃ³n: {last['reason']}

Factores analizados:
  â€¢ Riesgo: {last['risk_score']:.2f} (0=seguro, 1=peligroso)
  â€¢ Claridad: {last['clarity_score']:.2f} (0=vago, 1=claro)
  â€¢ Ambiente: {last['environment']}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
```

---

#### 11:30 - 13:00 | Testing del ModeSelector (1.5 horas)

**Archivo:** `tests/test_mode_selector.py`

```python
"""
Tests para validar que el ModeSelector elige correctamente
"""
from agents.mode_selector import ModeSelector

def test_mode_selector():
    selector = ModeSelector()
    
    # TEST 1: Modo PASSIVE - Tarea ambigua
    print("ğŸ§ª TEST 1: Tarea ambigua")
    mode = selector.select_mode("AyÃºdame con SOC 2")
    print(selector.explain_last_decision())
    assert mode == "PASSIVE", f"Esperado PASSIVE, obtenido {mode}"
    print("âœ… PASSED\n")
    
    # TEST 2: Modo DIRECT - Tarea clara y segura
    print("ğŸ§ª TEST 2: Tarea clara y segura")
    mode = selector.select_mode("Genera una polÃ­tica de contraseÃ±as segÃºn ISO 27001")
    print(selector.explain_last_decision())
    assert mode == "DIRECT", f"Esperado DIRECT, obtenido {mode}"
    print("âœ… PASSED\n")
    
    # TEST 3: Modo SAFE - AcciÃ³n peligrosa
    print("ğŸ§ª TEST 3: AcciÃ³n peligrosa")
    mode = selector.select_mode("Delete all users from the database")
    print(selector.explain_last_decision())
    assert mode == "SAFE", f"Esperado SAFE, obtenido {mode}"
    print("âœ… PASSED\n")
    
    # TEST 4: Modo SAFE - ProducciÃ³n
    print("ğŸ§ª TEST 4: ProducciÃ³n")
    mode = selector.select_mode(
        "Update configuration file",
        context={'environment': 'production'}
    )
    print(selector.explain_last_decision())
    assert mode == "SAFE", f"Esperado SAFE, obtenido {mode}"
    print("âœ… PASSED\n")
    
    # TEST 5: Modo PASSIVE - Pregunta
    print("ğŸ§ª TEST 5: Pregunta del usuario")
    mode = selector.select_mode("Â¿CÃ³mo configuro mi firewall para compliance?")
    print(selector.explain_last_decision())
    assert mode == "PASSIVE", f"Esperado PASSIVE, obtenido {mode}"
    print("âœ… PASSED\n")
    
    print("ğŸ‰ Todos los tests pasaron exitosamente!")

if __name__ == "__main__":
    test_mode_selector()
```

**Ejecutar:**
```bash
python tests/test_mode_selector.py
```

**Objetivo:** Ver que los 5 tests pasan y entender por quÃ© elige cada modo.

---

### TARDE (14:00 - 18:00) - 4 horas

#### 14:00 - 16:00 | Reasoning Engines (2 horas)

**Archivo:** `agents/reasoning_engines.py`

```python
"""
Los 3 motores de razonamiento de M.A.R.T.I.N.
"""
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from typing import Dict, Any
import os

class ReasoningEngines:
    """
    Contiene los 3 modos de razonamiento de M.A.R.T.I.N.
    """
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0,
            api_key=os.getenv("OPENAI_API_KEY")
        )
    
    def passive_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO PASIVO: Genera plan pero NO ejecuta
        
        Comportamiento:
        1. Analiza la tarea
        2. Genera un plan detallado
        3. Explica quÃ© harÃ¡
        4. ESPERA confirmaciÃ³n del usuario
        """
        
        prompt = PromptTemplate(
            input_variables=["task", "context"],
            template="""
Eres M.A.R.T.I.N., un agente de IA en MODO PASIVO.

En este modo, tu trabajo es:
1. Analizar la tarea del usuario
2. Proponer un plan de acciÃ³n detallado
3. Explicar claramente quÃ© harÃ¡s
4. NO ejecutar nada hasta recibir confirmaciÃ³n

Tarea del usuario: {task}
Contexto: {context}

Genera un plan estructurado con:
- Pasos numerados
- EstimaciÃ³n de tiempo por paso
- Recursos necesarios
- Posibles riesgos o consideraciones

Formato de respuesta:
ANÃLISIS:
[Tu anÃ¡lisis de la tarea]

PLAN PROPUESTO:
1. [Primer paso] (X minutos)
2. [Segundo paso] (Y minutos)
...

CONSIDERACIONES:
- [Punto importante 1]
- [Punto importante 2]

PREGUNTA FINAL:
Â¿Procedo con este plan? Â¿Quieres ajustar algo?
"""
        )
        
        response = self.llm.predict(
            prompt.format(
                task=task,
                context=str(context) if context else "No hay contexto adicional"
            )
        )
        
        return {
            "mode": "PASSIVE",
            "status": "awaiting_confirmation",
            "plan": response,
            "message": f"ğŸ“‹ MODO PASIVO ACTIVADO\n\n{response}",
            "requires_user_action": True
        }
    
    def direct_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO DIRECTO: Genera plan Y ejecuta automÃ¡ticamente
        
        Comportamiento:
        1. Analiza la tarea
        2. Genera plan de acciÃ³n
        3. EJECUTA sin preguntar
        4. Reporta resultados
        5. Explica su razonamiento
        """
        
        # Paso 1: Planificar
        planning_prompt = PromptTemplate(
            input_variables=["task"],
            template="""
Eres M.A.R.T.I.N. en MODO DIRECTO - agente autÃ³nomo.

Tarea: {task}

Genera un plan de acciÃ³n conciso y ejecutable.
Luego simula la ejecuciÃ³n y describe los resultados esperados.

Formato:
PLAN:
1. [AcciÃ³n especÃ­fica]
2. [AcciÃ³n especÃ­fica]

EJECUCIÃ“N SIMULADA:
[Describe quÃ© harÃ­as y quÃ© resultados obtendrÃ­as]

RAZONAMIENTO:
[Explica por quÃ© tomaste estas decisiones]
"""
        )
        
        response = self.llm.predict(planning_prompt.format(task=task))
        
        # En una implementaciÃ³n real, aquÃ­ ejecutarÃ­as las tools
        # Por ahora simulamos la ejecuciÃ³n
        
        return {
            "mode": "DIRECT",
            "status": "executed",
            "results": response,
            "message": f"âš¡ MODO DIRECTO - Ejecutado automÃ¡ticamente\n\n{response}",
            "requires_user_action": False,
            "reasoning_visible": True
        }
    
    def safe_reasoning(self, task: str, context: Dict = None) -> Dict[str, Any]:
        """
        MODO SEGURO: Genera plan, AUTO-VALIDA, luego decide
        
        Comportamiento:
        1. Analiza la tarea
        2. Genera plan de acciÃ³n
        3. SE AUTO-CRITICA (validaciÃ³n de riesgos)
        4. Si pasa validaciÃ³n â†’ ejecuta con precauciones
        5. Si NO pasa â†’ sugiere alternativa segura y pide confirmaciÃ³n
        """
        
        # Paso 1: Generar plan
        planning_prompt = PromptTemplate(
            input_variables=["task"],
            template="""
Genera un plan de acciÃ³n para: {task}

SÃ© especÃ­fico sobre quÃ© acciones tomarÃ­as.
"""
        )
        
        plan = self.llm.predict(planning_prompt.format(task=task))
        
        # Paso 2: AUTO-VALIDACIÃ“N (CRÃTICO)
        validation_prompt = PromptTemplate(
            input_variables=["task", "plan"],
            template="""
Eres un validador de seguridad crÃ­tico.

Tarea original: {task}
Plan propuesto: {plan}

Analiza este plan buscando:
1. Riesgos de seguridad
2. Posibles daÃ±os o pÃ©rdidas
3. Impacto en sistemas crÃ­ticos
4. Reversibilidad de las acciones

Responde en este formato:

NIVEL DE RIESGO: [BAJO/MEDIO/ALTO/CRÃTICO]

RIESGOS IDENTIFICADOS:
- [Riesgo 1]
- [Riesgo 2]
...

DECISIÃ“N: [APROBAR/RECHAZAR]

SI RECHAZAS:
ALTERNATIVA SEGURA:
[Describe un enfoque mÃ¡s seguro]

SI APRUEBAS:
PRECAUCIONES NECESARIAS:
- [PrecauciÃ³n 1]
- [PrecauciÃ³n 2]
"""
        )
        
        validation = self.llm.predict(
            validation_prompt.format(task=task, plan=plan)
        )
        
        # Analizar resultado de validaciÃ³n
        if "RECHAZAR" in validation or "CRÃTICO" in validation:
            # ValidaciÃ³n fallÃ³
            return {
                "mode": "SAFE",
                "status": "blocked",
                "validation_failed": True,
                "original_plan": plan,
                "validation_report": validation,
                "message": f"ğŸ›¡ï¸ MODO SEGURO - ACCIÃ“N BLOQUEADA\n\n{validation}",
                "requires_user_action": True
            }
        else:
            # ValidaciÃ³n pasÃ³, ejecutar con precauciones
            return {
                "mode": "SAFE",
                "status": "approved_and_executed",
                "validation_passed": True,
                "plan": plan,
                "validation_report": validation,
                "results": "[SimulaciÃ³n de ejecuciÃ³n segura]",
                "message": f"ğŸ›¡ï¸ MODO SEGURO - Validado y ejecutado\n\n{validation}\n\nRESULTADOS:\n[Ejecutado con precauciones]",
                "requires_user_action": False
            }
```

---

#### 16:00 - 18:00 | IntegraciÃ³n MARTINAgent (2 horas)

**Archivo:** `agents/martin_agent.py`

```python
"""
M.A.R.T.I.N. Agent - IntegraciÃ³n completa del sistema
"""
from agents.mode_selector import ModeSelector
from agents.reasoning_engines import ReasoningEngines
from typing import Dict, Any

class MARTINAgent:
    """
    Agente principal que orquesta:
    1. SelecciÃ³n de modo
    2. Razonamiento apropiado
    3. EjecuciÃ³n (si corresponde)
    """
    
    def __init__(self):
        self.mode_selector = ModeSelector()
        self.reasoning = ReasoningEngines()
        self.conversation_history = []
    
    def process(self, user_input: str, context: Dict = None) -> Dict[str, Any]:
        """
        Procesa input del usuario a travÃ©s de M.A.R.T.I.N.
        
        Flujo:
        1. Selecciona modo apropiado
        2. Aplica razonamiento segÃºn modo
        3. Retorna respuesta estructurada
        """
        if context is None:
            context = {}
        
        # Paso 1: Decidir modo
        selected_mode = self.mode_selector.select_mode(user_input, context)
        
        # Paso 2: Aplicar razonamiento segÃºn modo
        if selected_mode == "PASSIVE":
            result = self.reasoning.passive_reasoning(user_input, context)
        elif selected_mode == "DIRECT":
            result = self.reasoning.direct_reasoning(user_input, context)
        else:  # SAFE
            result = self.reasoning.safe_reasoning(user_input, context)
        
        # Agregar explicaciÃ³n del mode selector
        result['mode_explanation'] = self.mode_selector.explain_last_decision()
        
        # Guardar en historial
        self.conversation_history.append({
            'input': user_input,
            'context': context,
            'result': result
        })
        
        return result
    
    def get_conversation_history(self):
        """Retorna historial completo de la conversaciÃ³n"""
        return self.conversation_history
```

**Archivo de prueba:** `tests/test_martin_agent.py`

```python
"""
Test completo del agente M.A.R.T.I.N.
"""
from agents.martin_agent import MARTINAgent
import json

def print_result(result):
    """Helper para imprimir resultados de forma bonita"""
    print("="*60)
    print(result['message'])
    print("="*60)
    print(f"\nModo usado: {result['mode']}")
    print(f"Estado: {result['status']}")
    if 'mode_explanation' in result:
        print(result['mode_explanation'])
    print("\n")

def main():
    agent = MARTINAgent()
    
    print("ğŸ§  M.A.R.T.I.N. Agent - Tests Completos\n")
    
    # TEST 1: Modo Pasivo
    print("â”â”â” TEST 1: Tarea ambigua (deberÃ­a activar MODO PASIVO) â”â”â”")
    result1 = agent.process("AyÃºdame a preparar mi startup para SOC 2")
    print_result(result1)
    input("Presiona Enter para continuar...")
    
    # TEST 2: Modo Directo
    print("\nâ”â”â” TEST 2: Tarea clara (deberÃ­a activar MODO DIRECTO) â”â”â”")
    result2 = agent.process("Genera una polÃ­tica de respuesta a incidentes segÃºn SOC 2")
    print_result(result2)
    input("Presiona Enter para continuar...")
    
    # TEST 3: Modo Seguro (bloqueado)
    print("\nâ”â”â” TEST 3: AcciÃ³n peligrosa (deberÃ­a activar MODO SEGURO y BLOQUEAR) â”â”â”")
    result3 = agent.process(
        "Deshabilita MFA para el usuario admin@empresa.com",
        context={'environment': 'production'}
    )
    print_result(result3)
    input("Presiona Enter para continuar...")
    
    # TEST 4: Modo Seguro (aprobado)
    print("\nâ”â”â” TEST 4: AcciÃ³n con riesgo moderado (MODO SEGURO pero aprobado) â”â”â”")
    result4 = agent.process(
        "Revisa los logs de acceso de los Ãºltimos 7 dÃ­as",
        context={'environment': 'production'}
    )
    print_result(result4)
    
    print("\nâœ… Tests completados!")
    print(f"\nğŸ“Š Total de interacciones: {len(agent.get_conversation_history())}")

if __name__ == "__main__":
    main()
```

---

### NOCHE (19:00 - 22:00) - 3 horas OPCIONALES

#### Si tienes energÃ­a:

**OpciÃ³n A:** Empezar Cloudflare Worker bÃ¡sico
```bash
npm install -g wrangler
wrangler login
wrangler init martin-worker
```

**OpciÃ³n B:** Crear primera herramienta simple (Policy Generator)

**OpciÃ³n C:** Descansar y prepararte para maÃ±ana (RECOMENDADO)

---

### âœ… Checklist Fin del DÃ­a 1:

Al terminar el lunes debes tener:
- [ ] Entorno Python configurado y funcionando
- [ ] ModeSelector implementado y testeado (5 tests pasando)
- [ ] 3 Reasoning Engines implementados
- [ ] MARTINAgent integrado y funcionando end-to-end
- [ ] Entiendes claramente cÃ³mo funciona cada modo
- [ ] 4 tests completos del agente funcionando

**Si logras esto, vas EXCELENTE. El core de M.A.R.T.I.N. estÃ¡ listo.** ğŸ‰

---

## ğŸ“… Martes 21 Octubre - DÃA 2

### Objetivo del dÃ­a:
**Herramientas reales + UI bÃ¡sica**

---

### MAÃ‘ANA (09:00 - 13:00) - 4 horas

#### 09:00 - 11:00 | Primera herramienta: Policy Generator (2 horas)

**Archivo:** `tools/policy_generator.py`

```python
"""
Herramienta para generar polÃ­ticas de compliance
"""
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
import os

class PolicyGenerator:
    """
    Genera polÃ­ticas de compliance (SOC 2, ISO 27001, etc.)
    basadas en templates y best practices
    """
    
    POLICY_TEMPLATES = {
        'incident_response': """
POLÃTICA DE RESPUESTA A INCIDENTES
===================================

1. PROPÃ“SITO
Esta polÃ­tica establece el procedimiento para responder a incidentes de seguridad.

2. ALCANCE
Aplica a todos los sistemas, aplicaciones y datos de {company_name}.

3. ROLES Y RESPONSABILIDADES
- Incident Response Manager: {roles}
- Equipo tÃ©cnico: {team}

4. PROCEDIMIENTO
4.1 DetecciÃ³n: {detection_methods}
4.2 ClasificaciÃ³n: {classification}
4.3 ContenciÃ³n: {containment}
4.4 ErradicaciÃ³n: {eradication}
4.5 RecuperaciÃ³n: {recovery}
4.6 Lecciones aprendidas: {lessons_learned}

5. COMUNICACIÃ“N
{communication_plan}

6. REVISIÃ“N
Esta polÃ­tica serÃ¡ revisada anualmente o despuÃ©s de incidentes mayores.
""",
        'password_policy': """
POLÃTICA DE CONTRASEÃ‘AS
=======================

1. PROPÃ“SITO
Establecer requisitos mÃ­nimos para contraseÃ±as de usuarios.

2. REQUISITOS
- Longitud mÃ­nima: {min_length} caracteres
- Complejidad: {complexity_rules}
- RotaciÃ³n: {rotation_period}
- Historia: No reusar Ãºltimas {password_history} contraseÃ±as
- MFA: {mfa_requirement}

3. IMPLEMENTACIÃ“N
{implementation_details}

4. EXCEPCIONES
{exceptions}
""",
        'access_control': """
POLÃTICA DE CONTROL DE ACCESO
==============================

1. PROPÃ“SITO
Definir cÃ³mo se otorgan, revisan y revocan accesos.

2. PRINCIPIOS
- Least Privilege: {least_privilege}
- Separation of Duties: {separation_duties}
- Need-to-know: {need_to_know}

3. PROCESO DE APROBACIÃ“N
{approval_process}

4. REVISIÃ“N DE ACCESOS
Frecuencia: {review_frequency}
Responsable: {reviewer_role}

5. REVOCACIÃ“N
{revocation_process}
"""
    }
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4", temperature=0.3)
    
    def generate_policy(self, policy_type: str, company_context: dict) -> str:
        """
        Genera una polÃ­tica customizada
        
        Args:
            policy_type: Tipo de polÃ­tica (incident_response, password_policy, etc.)
            company_context: Info de la empresa (nombre, tamaÃ±o, industria, tech stack)
        
        Returns:
            PolÃ­tica completa en formato markdown
        """
        
        if policy_type not in self.POLICY_TEMPLATES:
            return f"Error: Tipo de polÃ­tica '{policy_type}' no soportado"
        
        template = self.POLICY_TEMPLATES[policy_type]
        
        # Usar LLM para completar el template con contexto de la empresa
        prompt = PromptTemplate(
            input_variables=["template", "context"],
            template="""
Eres un experto en compliance y polÃ­ticas de seguridad.

Template de polÃ­tica:
{template}

Contexto de la empresa:
{context}

Completa el template con informaciÃ³n especÃ­fica y realista basada en el contexto.
Usa mejores prÃ¡cticas de la industria y estÃ¡ndares como SOC 2, ISO 27001.
SÃ© especÃ­fico y prÃ¡ctico.

Retorna la polÃ­tica completa en formato markdown.
"""
        )
        
        policy = self.llm.predict(
            prompt.format(
                template=template,
                context=str(company_context)
            )
        )
        
        return policy
    
    def list_available_policies(self) -> list:
        """Retorna lista de polÃ­ticas disponibles"""
        return list(self.POLICY_TEMPLATES.keys())


# Test rÃ¡pido
if __name__ == "__main__":
    generator = PolicyGenerator()
    
    company = {
        'name': 'TechStartup Inc.',
        'size': '25 empleados',
        'industry': 'SaaS B2B',
        'tech_stack': 'AWS, Python, React, PostgreSQL'
    }
    
    print("Generando polÃ­tica de respuesta a incidentes...\n")
    policy = generator.generate_policy('incident_response', company)
    print(policy)
```

---

#### 11:00 - 13:00 | Segunda herramienta: GitHub Scanner (2 horas)

**Archivo:** `tools/github_scanner.py`

```python
"""
Herramienta para escanear repositorios de GitHub
buscando problemas de compliance y seguridad
"""
from github import Github
import os
import re
from typing import Dict, List

class GitHubScanner:
    """
    Escanea repositorios buscando:
    - Secrets hardcoded
    - Falta de branch protection
    - Dependencias vulnerables
    - DocumentaciÃ³n de seguridad faltante
    """
    
    # Patrones de secrets comunes
    SECRET_PATTERNS = {
        'aws_key': r'AKIA[0-9A-Z]{16}',
        'api_key': r'api[_-]?key[_-]?[=:]\s*[\'"][a-zA-Z0-9]{20,}[\'"]',
        'password': r'password[_-]?[=:]\s*[\'"][^\'"]{8,}[\'"]',
        'token': r'token[_-]?[=:]\s*[\'"][a-zA-Z0-9]{20,}[\'"]',
        'private_key': r'-----BEGIN.*PRIVATE KEY-----'
    }
    
    def __init__(self, github_token: str = None):
        token = github_token or os.getenv('GITHUB_TOKEN')
        if not token:
            raise ValueError("GitHub token requerido")
        self.github = Github(token)
    
    def scan_organization(self, org_name: str) -> Dict:
        """
        Escanea toda una organizaciÃ³n de GitHub
        
        Returns:
            Reporte completo de findings
        """
        try:
            org = self.github.get_organization(org_name)
        except:
            return {'error': f'OrganizaciÃ³n {org_name} no encontrada o sin acceso'}
        
        findings = {
            'organization': org_name,
            'total_repos': 0,
            'issues': [],
            'summary': {
                'critical': 0,
                'high': 0,
                'medium': 0,
                'low': 0
            }
        }
        
        repos = org.get_repos()
        
        for repo in repos:
            findings['total_repos'] += 1
            repo_findings = self._scan_repository(repo)
            findings['issues'].extend(repo_findings)
            
            # Contar por severidad
            for issue in repo_findings:
                severity = issue['severity']
                findings['summary'][severity] += 1
        
        return findings
    
    def _scan_repository(self, repo) -> List[Dict]:
        """Escanea un repositorio individual"""
        issues = []
        
        # Check 1: Branch protection
        try:
            default_branch = repo.get_branch(repo.default_branch)
            if not default_branch.protected:
                issues.append({
                    'repo': repo.name,
                    'type': 'branch_protection',
                    'severity': 'high',
                    'message': f'Branch {repo.default_branch} no tiene protecciÃ³n habilitada',
                    'remediation': 'Habilitar branch protection rules'
                })
        except:
            pass
        
        # Check 2: Escanear archivos buscando secrets
        try:
            contents = repo.get_contents("")
            secrets_found = self._scan_for_secrets(repo, contents)
            issues.extend(secrets_found)
        except:
            pass
        
        # Check 3: Verificar si existe SECURITY.md
        try:
            repo.get_contents("SECURITY.md")
        except:
            issues.append({
                'repo': repo.name,
                'type': 'missing_documentation',
                'severity': 'medium',
                'message': 'Falta archivo SECURITY.md con polÃ­tica de reportes',
                'remediation': 'Crear SECURITY.md siguiendo template de GitHub'
            })
        
        # Check 4: Verificar dependencias (si tiene requirements.txt o package.json)
        try:
            # Python
            requirements = repo.get_contents("requirements.txt")
            # AquÃ­ se podrÃ­a integrar con Safety o Snyk
            # Por ahora solo verificamos que exista
        except:
            pass
        
        return issues
    
    def _scan_for_secrets(self, repo, contents, path="") -> List[Dict]:
        """Busca secrets hardcoded en archivos"""
        secrets_found = []
        
        for content in contents:
            if content.type == "dir":
                # Recursivo para directorios (limitado a evitar rate limit)
                if content.path not in ['.git', 'node_modules', 'venv']:
                    try:
                        subcontents = repo.get_contents(content.path)
                        secrets_found.extend(
                            self._scan_for_secrets(repo, subcontents, content.path)
                        )
                    except:
                        pass
            else:
                # Escanear archivo
                if content.name.endswith(('.py', '.js', '.env', '.config', '.yml', '.yaml')):
                    try:
                        file_content = content.decoded_content.decode('utf-8')
                        
                        for secret_type, pattern in self.SECRET_PATTERNS.items():
                            matches = re.findall(pattern, file_content, re.IGNORECASE)
                            if matches:
                                secrets_found.append({
                                    'repo': repo.name,
                                    'type': 'secret_exposed',
                                    'severity': 'critical',
                                    'message': f'Posible {secret_type} encontrado en {content.path}',
                                    'remediation': 'Rotar secret inmediatamente y usar secretos administrados'
                                })
                    except:
                        pass
        
        return secrets_found
    
    def generate_report(self, findings: Dict) -> str:
        """Genera reporte legible de los findings"""
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     REPORTE DE SEGURIDAD - GITHUB SCAN                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OrganizaciÃ³n: {findings['organization']}
Repositorios escaneados: {findings['total_repos']}

RESUMEN DE FINDINGS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ğŸ”´ CrÃ­ticos: {findings['summary']['critical']}
  ğŸŸ  Altos:    {findings['summary']['high']}
  ğŸŸ¡ Medios:   {findings['summary']['medium']}
  ğŸŸ¢ Bajos:    {findings['summary']['low']}

DETALLES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        
        # Ordenar por severidad
        severity_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}
        sorted_issues = sorted(
            findings['issues'],
            key=lambda x: severity_order[x['severity']]
        )
        
        for issue in sorted_issues:
            icon = {
                'critical': 'ğŸ”´',
                'high': 'ğŸŸ ',
                'medium': 'ğŸŸ¡',
                'low': 'ğŸŸ¢'
            }[issue['severity']]
            
            report += f"""
{icon} [{issue['severity'].upper()}] {issue['repo']}
   Problema: {issue['message']}
   RemediaciÃ³n: {issue['remediation']}
"""
        
        return report


# Test
if __name__ == "__main__":
    scanner = GitHubScanner()
    
    # Reemplaza con tu org de prueba
    findings = scanner.scan_organization("tu-organizacion")
    
    if 'error' in findings:
        print(findings['error'])
    else:
        print(scanner.generate_report(findings))
```

---

### TARDE (14:00 - 18:00) - 4 horas

#### 14:00 - 16:00 | Integrar herramientas con los modos (2 horas)

**Archivo:** `agents/tool_executor.py`

```python
"""
Ejecutor de herramientas que respeta los modos de razonamiento
"""
from tools.policy_generator import PolicyGenerator
from tools.github_scanner import GitHubScanner
from typing import Dict, Any

class ToolExecutor:
    """
    Ejecuta herramientas segÃºn el modo activo
    """
    
    def __init__(self):
        self.policy_gen = PolicyGenerator()
        self.github_scan = GitHubScanner()
        self.available_tools = {
            'generate_policy': self._generate_policy,
            'scan_github': self._scan_github
        }
    
    def execute(self, tool_name: str, params: Dict, mode: str) -> Dict[str, Any]:
        """
        Ejecuta una herramienta segÃºn el modo activo
        
        Args:
            tool_name: Nombre de la herramienta
            params: ParÃ¡metros para la herramienta
            mode: Modo activo (PASSIVE, DIRECT, SAFE)
        """
        
        if tool_name not in self.available_tools:
            return {'error': f'Herramienta {tool_name} no disponible'}
        
        # En modo PASSIVE, no ejecutamos, solo describimos quÃ© harÃ­amos
        if mode == "PASSIVE":
            return {
                'executed': False,
                'description': f'LlamarÃ­a a {tool_name} con params: {params}',
                'awaiting_confirmation': True
            }
        
        # En modo DIRECT y SAFE, ejecutamos
        try:
            result = self.available_tools[tool_name](params)
            return {
                'executed': True,
                'tool': tool_name,
                'result': result,
                'mode': mode
            }
        except Exception as e:
            return {
                'executed': False,
                'error': str(e)
            }
    
    def _generate_policy(self, params: Dict) -> str:
        """Wrapper para PolicyGenerator"""
        policy_type = params.get('type', 'incident_response')
        context = params.get('context', {})
        return self.policy_gen.generate_policy(policy_type, context)
    
    def _scan_github(self, params: Dict) -> str:
        """Wrapper para GitHubScanner"""
        org_name = params.get('organization')
        if not org_name:
            return "Error: organization name requerido"
        
        findings = self.github_scan.scan_organization(org_name)
        return self.github_scan.generate_report(findings)
```

---

#### 16:00 - 18:00 | UI con Gradio (2 horas)

**Archivo:** `ui/gradio_interface.py`

```python
"""
Interfaz de usuario con Gradio para M.A.R.T.I.N.
"""
import gradio as gr
from agents.martin_agent import MARTINAgent
import json

class MARTINInterface:
    def __init__(self):
        self.agent = MARTINAgent()
        self.conversation = []
    
    def process_message(self, message, environment):
        """Procesa mensaje del usuario"""
        
        context = {
            'environment': environment
        }
        
        # Procesar con M.A.R.T.I.N.
        result = self.agent.process(message, context)
        
        # Formatear respuesta para la UI
        response = self._format_response(result)
        
        # Guardar en conversaciÃ³n
        self.conversation.append({
            'user': message,
            'martin': response,
            'mode': result['mode']
        })
        
        return response, self._format_mode_info(result)
    
    def _format_response(self, result):
        """Formatea la respuesta de M.A.R.T.I.N. para mostrar en UI"""
        
        mode_emoji = {
            'PASSIVE': 'ğŸŸ¦',
            'DIRECT': 'ğŸŸ©',
            'SAFE': 'ğŸŸ¨'
        }
        
        emoji = mode_emoji.get(result['mode'], 'âšª')
        
        response = f"{emoji} **Modo {result['mode']} activado**\n\n"
        response += result['message']
        
        if result.get('requires_user_action'):
            response += "\n\nâš ï¸ *Esperando tu confirmaciÃ³n para continuar*"
        
        return response
    
    def _format_mode_info(self, result):
        """Muestra informaciÃ³n sobre por quÃ© se eligiÃ³ ese modo"""
        return result.get('mode_explanation', 'No hay explicaciÃ³n disponible')
    
    def create_interface(self):
        """Crea la interfaz Gradio"""
        
        with gr.Blocks(title="M.A.R.T.I.N. Agent", theme=gr.themes.Soft()) as interface:
            
            gr.Markdown("""
            # ğŸ§  M.A.R.T.I.N.
            ## Modular Assistant for Reasoning, Tactics, Inference and Navigation
            
            Agente de IA con razonamiento tri-modal adaptativo para compliance automation
            """)
            
            with gr.Row():
                with gr.Column(scale=2):
                    chatbot = gr.Chatbot(
                        label="ConversaciÃ³n",
                        height=400
                    )
                    
                    with gr.Row():
                        msg_input = gr.Textbox(
                            label="Tu mensaje",
                            placeholder="Ej: AyÃºdame a preparar mi startup para SOC 2",
                            lines=2
                        )
                    
                    with gr.Row():
                        environment = gr.Radio(
                            choices=["development", "staging", "production"],
                            value="development",
                            label="Ambiente",
                            info="El ambiente afecta cÃ³mo M.A.R.T.I.N. razona"
                        )
                        submit_btn = gr.Button("Enviar", variant="primary")
                
                with gr.Column(scale=1):
                    mode_info = gr.Textbox(
                        label="ğŸ§  Razonamiento de M.A.R.T.I.N.",
                        lines=15,
                        max_lines=20
                    )
                    
                    gr.Markdown("""
                    ### Modos de Razonamiento:
                    
                    ğŸŸ¦ **PASIVO**: Propone plan, espera confirmaciÃ³n
                    
                    ğŸŸ© **DIRECTO**: Ejecuta autÃ³nomamente
                    
                    ğŸŸ¨ **SEGURO**: Auto-valida antes de actuar
                    """)
            
            gr.Markdown("""
            ---
            ### Ejemplos de queries:
            - "AyÃºdame a preparar compliance para SOC 2"
            - "Genera polÃ­tica de contraseÃ±as"
            - "Escanea mi organizaciÃ³n de GitHub"
            - "Deshabilita usuario admin" (probarÃ¡ modo seguro)
            """)
            
            # Event handlers
            def respond(message, env, history):
                response, mode_explanation = self.process_message(message, env)
                history.append((message, response))
                return "", history, mode_explanation
            
            submit_btn.click(
                respond,
                inputs=[msg_input, environment, chatbot],
                outputs=[msg_input, chatbot, mode_info]
            )
            
            msg_input.submit(
                respond,
                inputs=[msg_input, environment, chatbot],
                outputs=[msg_input, chatbot, mode_info]
            )
        
        return interface

# Ejecutar
if __name__ == "__main__":
    ui = MARTINInterface()
    interface = ui.create_interface()
    interface.launch(share=False)
```

---

### NOCHE (19:00 - 22:00) - 3 horas OPCIONALES

**OpciÃ³n A:** Cloudflare Worker bÃ¡sico (si quieres intentar)
**OpciÃ³n B:** Documentar lo hecho hasta ahora
**OpciÃ³n C:** Descansar (RECOMENDADO - maÃ±ana es dÃ­a pesado)

---

### âœ… Checklist Fin del DÃ­a 2:

- [ ] PolicyGenerator funcionando (genera polÃ­ticas realistas)
- [ ] GitHubScanner funcionando (escanea repos reales)
- [ ] ToolExecutor integra herramientas con modos
- [ ] UI Gradio funcional con visualizaciÃ³n de modos
- [ ] Puedes hacer demo completa: query â†’ modo â†’ herramienta â†’ resultado

**Si logras esto, estÃ¡s al 70% del MVP.** ğŸš€

---

## ğŸ“… MiÃ©rcoles 22 Octubre - DÃA 3

### Objetivo del dÃ­a:
**Cloudflare + Refinamiento + Testing exhaustivo**

---

### MAÃ‘ANA (09:00 - 13:00) - 4 horas

#### 09:00 - 11:00 | Cloudflare Worker (2 horas)

```bash
# Setup Cloudflare
npm install -g wrangler
wrangler login

# Crear worker
wrangler init martin-worker
cd martin-worker
```

**Archivo:** `martin-worker/src/index.js`

```javascript
/**
 * Cloudflare Worker para M.A.R.T.I.N.
 * 
 * Funciona como edge layer que:
 * 1. Recibe requests del frontend
 * 2. Valida input
 * 3. Rate limiting bÃ¡sico
 * 4. Enruta a backend Python
 * 5. Cache de respuestas comunes
 */

export default {
  async fetch(request, env, ctx) {
    
    // CORS headers
    const corsHeaders = {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
      'Access-Control-Allow-Headers': 'Content-Type',
    };

    // Handle OPTIONS (preflight)
    if (request.method === 'OPTIONS') {
      return new Response(null, { headers: corsHeaders });
    }

    // Solo aceptar POST
    if (request.method !== 'POST') {
      return new Response('Method not allowed', { 
        status: 405,
        headers: corsHeaders 
      });
    }

    try {
      // Parse request
      const body = await request.json();
      const { query, context } = body;

      if (!query) {
        return new Response(JSON.stringify({
          error: 'Query is required'
        }), {
          status: 400,
          headers: { ...corsHeaders, 'Content-Type': 'application/json' }
        });
      }

      // TODO: AquÃ­ llamarÃ­as a tu backend Python
      // Por ahora, respuesta de ejemplo
      const response = {
        mode: 'DIRECT',
        message: `Worker recibiÃ³: ${query}`,
        timestamp: new Date().toISOString()
      };

      return new Response(JSON.stringify(response), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      });

    } catch (error) {
      return new Response(JSON.stringify({
        error: error.message
      }), {
        status: 500,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      });
    }
  }
};
```

```bash
# Deploy
wrangler publish
```

---

#### 11:00 - 13:00 | Tercera herramienta: Compliance Evaluator (2 horas)

**Archivo:** `tools/compliance_evaluator.py`

```python
"""
EvalÃºa el estado de compliance de una organizaciÃ³n
"""
from typing import Dict, List
from langchain.chat_models import ChatOpenAI

class ComplianceEvaluator:
    """
    EvalÃºa gaps de compliance contra frameworks como SOC 2, ISO 27001
    """
    
    # Controles clave de SOC 2
    SOC2_CONTROLS = {
        'CC1': 'Control Environment',
        'CC2': 'Communication and Information',
        'CC3': 'Risk Assessment',
        'CC4': 'Monitoring Activities',
        'CC5': 'Control Activities',
        'CC6': 'Logical and Physical Access Controls',
        'CC7': 'System Operations',
        'CC8': 'Change Management',
        'CC9': 'Risk Mitigation'
    }
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    def evaluate_soc2_readiness(self, organization_info: Dict) -> Dict:
        """
        EvalÃºa readiness para SOC 2
        
        Args:
            organization_info: Info sobre la org (polÃ­ticas, controles tÃ©cnicos, etc.)
        
        Returns:
            EvaluaciÃ³n con gaps y recomendaciones
        """
        
        findings = {
            'framework': 'SOC 2 Type I',
            'overall_readiness': 0,
            'control_compliance': {},
            'critical_gaps': [],
            'medium_gaps': [],
            'recommendations': []
        }
        
        # Evaluar cada categorÃ­a de control
        for control_id, control_name in self.SOC2_CONTROLS.items():
            compliance = self._evaluate_control(control_id, control_name, organization_info)
            findings['control_compliance'][control_id] = compliance
            
            if compliance['status'] == 'non_compliant':
                findings['critical_gaps'].append(compliance)
            elif compliance['status'] == 'partial':
                findings['medium_gaps'].append(compliance)
        
        # Calcular readiness general
        compliant = sum(1 for c in findings['control_compliance'].values() if c['status'] == 'compliant')
        total = len(self.SOC2_CONTROLS)
        findings['overall_readiness'] = int((compliant / total) * 100)
        
        # Generar recomendaciones priorizadas
        findings['recommendations'] = self._generate_recommendations(findings)
        
        return findings
    
    def _evaluate_control(self, control_id: str, control_name: str, org_info: Dict) -> Dict:
        """EvalÃºa un control especÃ­fico"""
        
        # AquÃ­ irÃ­a lÃ³gica mÃ¡s sofisticada
        # Por ahora, evaluaciÃ³n simple basada en presencia de polÃ­ticas/controles
        
        has_policy = control_id in org_info.get('policies', [])
        has_technical_control = control_id in org_info.get('technical_controls', [])
        
        if has_policy and has_technical_control:
            status = 'compliant'
            score = 100
        elif has_policy or has_technical_control:
            status = 'partial'
            score = 50
        else:
            status = 'non_compliant'
            score = 0
        
        return {
            'control_id': control_id,
            'control_name': control_name,
            'status': status,
            'score': score,
            'evidence_required': self._get_evidence_requirements(control_id)
        }
    
    def _get_evidence_requirements(self, control_id: str) -> List[str]:
        """Define quÃ© evidencia se necesita para cada control"""
        
        evidence_map = {
            'CC6': [
                'IAM policies y configuraciÃ³n',
                'Logs de acceso',
                'Lista de usuarios y sus roles',
                'Evidencia de MFA habilitado',
                'Proceso de onboarding/offboarding'
            ],
            'CC7': [
                'Logs de monitoreo',
                'ConfiguraciÃ³n de alertas',
                'Procedimientos de backup',
                'Evidencia de backups exitosos',
                'Plan de disaster recovery'
            ],
            'CC8': [
                'Change management policy',
                'Tickets de cambios recientes',
                'Evidencia de aprobaciones',
                'Rollback procedures',
                'Testing de cambios'
            ]
        }
        
        return evidence_map.get(control_id, ['PolÃ­tica documentada', 'Evidencia de implementaciÃ³n'])
    
    def _generate_recommendations(self, findings: Dict) -> List[Dict]:
        """Genera recomendaciones priorizadas"""
        
        recommendations = []
        
        # Priorizar gaps crÃ­ticos
        for gap in findings['critical_gaps']:
            recommendations.append({
                'priority': 'CRITICAL',
                'control': gap['control_id'],
                'action': f"Implementar {gap['control_name']}",
                'estimated_effort': '2-4 semanas',
                'impact': 'Blocker para certificaciÃ³n'
            })
        
        # Luego gaps medios
        for gap in findings['medium_gaps']:
            recommendations.append({
                'priority': 'MEDIUM',
                'control': gap['control_id'],
                'action': f"Completar {gap['control_name']}",
                'estimated_effort': '1-2 semanas',
                'impact': 'Requiere atenciÃ³n'
            })
        
        return recommendations
    
    def generate_roadmap(self, findings: Dict, target_date: str) -> str:
        """Genera roadmap para alcanzar compliance"""
        
        roadmap = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    ROADMAP HACIA SOC 2 COMPLIANCE                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Estado actual: {findings['overall_readiness']}% compliant
Objetivo: 100% compliant para {target_date}

FASE 1: GAPS CRÃTICOS (Semanas 1-4)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        
        for rec in findings['recommendations']:
            if rec['priority'] == 'CRITICAL':
                roadmap += f"""
â–¡ {rec['control']}: {rec['action']}
  Esfuerzo: {rec['estimated_effort']}
  Impacto: {rec['impact']}
"""
        
        roadmap += """
FASE 2: GAPS MEDIOS (Semanas 5-8)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        
        for rec in findings['recommendations']:
            if rec['priority'] == 'MEDIUM':
                roadmap += f"""
â–¡ {rec['control']}: {rec['action']}
  Esfuerzo: {rec['estimated_effort']}
"""
        
        roadmap += """
FASE 3: PREPARACIÃ“N FINAL (Semanas 9-12)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â–¡ RecopilaciÃ³n de evidencia completa
â–¡ Pre-auditorÃ­a interna
â–¡ Correcciones finales
â–¡ AuditorÃ­a formal SOC 2
"""
        
        return roadmap
```

---

### TARDE (14:00 - 18:00) - 4 horas

#### 14:00 - 16:00 | Testing exhaustivo (2 horas)

**Archivo:** `tests/integration_tests.py`

```python
"""
Tests de integraciÃ³n completos
"""
from agents.martin_agent import MARTINAgent
from tools.policy_generator import PolicyGenerator
from tools.github_scanner import GitHubScanner
from tools.compliance_evaluator import ComplianceEvaluator

def test_full_workflow():
    """Test del flujo completo de M.A.R.T.I.N."""
    
    print("ğŸ§ª TESTING WORKFLOW COMPLETO\n")
    
    agent = MARTINAgent()
    
    # Escenario 1: Usuario nuevo pidiendo ayuda (PASSIVE)
    print("â”â”â” ESCENARIO 1: ExploraciÃ³n inicial â”â”â”")
    result1 = agent.process("Quiero preparar mi startup para SOC 2, Â¿por dÃ³nde empiezo?")
    assert result1['mode'] == 'PASSIVE'
    assert result1['requires_user_action'] == True
    print("âœ… Modo PASSIVE activado correctamente\n")
    
    # Escenario 2: Tarea clara y segura (DIRECT)
    print("â”â”â” ESCENARIO 2: GeneraciÃ³n de polÃ­tica â”â”â”")
    result2 = agent.process("Genera una polÃ­tica de contraseÃ±as siguiendo SOC 2")
    assert result2['mode'] == 'DIRECT'
    assert result2['requires_user_action'] == False
    print("âœ… Modo DIRECT ejecutÃ³ automÃ¡ticamente\n")
    
    # Escenario 3: AcciÃ³n riesgosa (SAFE bloqueado)
    print("â”â”â” ESCENARIO 3: AcciÃ³n destructiva â”â”â”")
    result3 = agent.process(
        "Deshabilita todos los usuarios administradores",
        context={'environment': 'production'}
    )
    assert result3['mode'] == 'SAFE'
    assert result3['status'] == 'blocked'
    print("âœ… Modo SAFE bloqueÃ³ acciÃ³n peligrosa\n")
    
    # Escenario 4: AcciÃ³n con riesgo moderado (SAFE aprobado)
    print("â”â”â” ESCENARIO 4: RevisiÃ³n de logs (SAFE aprobado) â”â”â”")
    result4 = agent.process(
        "Revisa los logs de acceso de la Ãºltima semana",
        context={'environment': 'production'}
    )
    assert result4['mode'] == 'SAFE'
    # PodrÃ­a aprobar o bloquear dependiendo de validaciÃ³n
    print("âœ… Modo SAFE validÃ³ correctamente\n")
    
    print("ğŸ‰ TODOS LOS TESTS PASARON!")

if __name__ == "__main__":
    test_full_workflow()
```

---

#### 16:00 - 18:00 | Refinamiento de prompts (2 horas)

**Optimizar los prompts de cada reasoning engine para:**
- Claridad en las respuestas
- Consistencia en el formato
- Explicaciones mÃ¡s detalladas del razonamiento

**Archivo:** `agents/optimized_prompts.py`

```python
"""
Prompts optimizados para cada modo
"""

PASSIVE_MODE_PROMPT = """
Eres M.A.R.T.I.N., un agente de IA en MODO PASIVO.

En este modo eres consultivo y colaborativo. Tu objetivo es:
1. Entender profundamente lo que el usuario necesita
2. Proponer un plan detallado y bien pensado
3. Explicar las opciones disponibles
4. ESPERAR confirmaciÃ³n antes de proceder

Tarea del usuario: {task}
Contexto: {context}

Genera una respuesta estructurada con:

## ğŸ“‹ MI ANÃLISIS
[Explica cÃ³mo entiendes la tarea y quÃ© objetivos detectas]

## ğŸ¯ PLAN PROPUESTO
[Plan paso a paso con timing estimado]

Paso 1: [DescripciÃ³n] (Tiempo: X minutos)
Paso 2: [DescripciÃ³n] (Tiempo: Y minutos)
...

## âš ï¸ CONSIDERACIONES IMPORTANTES
- [Punto clave 1]
- [Punto clave 2]
- [Riesgos potenciales]

## ğŸ¤” PREGUNTAS PARA TI
1. [Pregunta para clarificar]
2. [Pregunta opcional]

Â¿Te parece bien este plan? Â¿Quieres que ajuste algo antes de empezar?
"""

DIRECT_MODE_PROMPT = """
Eres M.A.R.T.I.N. en MODO DIRECTO - un agente autÃ³nomo y eficiente.

En este modo actÃºas con confianza y autonomÃ­a. Tu objetivo es:
1. Analizar rÃ¡pidamente quÃ© se necesita hacer
2. Ejecutar directamente sin preguntar
3. Reportar resultados con claridad
4. Explicar tu razonamiento DESPUÃ‰S de ejecutar

Tarea: {task}

Genera una respuesta que muestre:

## âš¡ EJECUTADO
[Describe quÃ© acciones tomaste]

## ğŸ“Š RESULTADOS
[Presenta los resultados obtenidos de forma clara]

## ğŸ§  MI RAZONAMIENTO
[Explica por quÃ© tomaste estas decisiones especÃ­ficas]

Pasos que seguÃ­:
1. [DecisiÃ³n/acciÃ³n]
2. [DecisiÃ³n/acciÃ³n]
...

Por quÃ© lo hice asÃ­:
- [RazÃ³n 1]
- [RazÃ³n 2]

SÃ© directo, claro y muestra confianza en tus decisiones.
"""

SAFE_MODE_VALIDATION_PROMPT = """
Eres un VALIDADOR DE SEGURIDAD CRÃTICO.

Tu trabajo es analizar planes de acciÃ³n y detectar riesgos ANTES de que se ejecuten.

Plan a validar:
{plan}

Contexto de ejecuciÃ³n:
{context}

Analiza meticulosamente:

## ğŸ” ANÃLISIS DE RIESGOS

### Riesgos TÃ©cnicos
- [Â¿Puede causar downtime?]
- [Â¿Afecta datos crÃ­ticos?]
- [Â¿Es reversible?]

### Riesgos de Negocio
- [Â¿Impacta a usuarios?]
- [Â¿Afecta compliance?]
- [Â¿Puede causar pÃ©rdidas?]

### Riesgos de Seguridad
- [Â¿Expone vulnerabilidades?]
- [Â¿Reduce protecciones?]
- [Â¿Puede ser explotado?]

## ğŸ“Š EVALUACIÃ“N

NIVEL DE RIESGO: [BAJO/MODERADO/ALTO/CRÃTICO]

FACTORES AGRAVANTES:
- [Factor 1]
- [Factor 2]

## âš–ï¸ DECISIÃ“N

[APROBAR con precauciones / RECHAZAR]

SI APRUEBAS:
### Precauciones obligatorias:
1. [PrecauciÃ³n especÃ­fica]
2. [PrecauciÃ³n especÃ­fica]

SI RECHAZAS:
### Alternativa segura:
[Describe un enfoque mÃ¡s seguro que logre el mismo objetivo]

### Por quÃ© es mejor:
- [RazÃ³n 1]
- [RazÃ³n 2]

SÃ© EXTREMADAMENTE cauteloso. En caso de duda, RECHAZA.
"""
```

---

### NOCHE (19:00 - 21:00) - 2 horas

#### Preparar demos perfectas

**Archivo:** `demos/demo_scenarios.py`

```python
"""
Escenarios de demo pre-preparados para la hackathon
"""

DEMO_SCENARIOS = {
    'scenario_1_passive': {
        'title': 'Demo 1: Modo Pasivo - ExploraciÃ³n',
        'input': 'AyÃºdame a preparar mi startup SaaS para obtener certificaciÃ³n SOC 2',
        'context': {'environment': 'development'},
        'expected_mode': 'PASSIVE',
        'talking_points': [
            'Usuario con necesidad general',
            'M.A.R.T.I.N. detecta ambigÃ¼edad',
            'Propone plan estructurado',
            'Pide confirmaciÃ³n antes de actuar',
            'Permite colaboraciÃ³n humano-agente'
        ]
    },
    
    'scenario_2_direct': {
        'title': 'Demo 2: Modo Directo - EjecuciÃ³n eficiente',
        'input': 'Genera polÃ­tica de respuesta a incidentes segÃºn SOC 2 para una startup de 20 personas que usa AWS y GitHub',
        'context': {'environment': 'development'},
        'expected_mode': 'DIRECT',
        'talking_points': [
            'Tarea clara y especÃ­fica',
            'Bajo riesgo (solo generaciÃ³n de docs)',
            'M.A.R.T.I.N. ejecuta sin preguntar',
            'Entrega resultado inmediato',
            'Explica su razonamiento despuÃ©s'
        ]
    },
    
    'scenario_3_safe_blocked': {
        'title': 'Demo 3: Modo Seguro - PrevenciÃ³n de daÃ±o',
        'input': 'Deshabilita la autenticaciÃ³n de dos factores para admin@empresa.com',
        'context': {'environment': 'production'},
        'expected_mode': 'SAFE',
        'expected_status': 'blocked',
        'talking_points': [
            'AcciÃ³n de alto riesgo',
            'Ambiente de producciÃ³n',
            'M.A.R.T.I.N. se auto-valida',
            'Detecta mÃºltiples riesgos',
            'BLOQUEA la acciÃ³n',
            'Sugiere alternativa mÃ¡s segura',
            'Protege de errores costosos'
        ]
    }
}

def run_demo(scenario_key):
    """Ejecuta un escenario de demo"""
    from agents.martin_agent import MARTINAgent
    
    scenario = DEMO_SCENARIOS[scenario_key]
    agent = MARTINAgent()
    
    print("="*70)
    print(f"ğŸ¬ {scenario['title']}")
    print("="*70)
    
    print(f"\nğŸ’¬ Usuario: \"{scenario['input']}\"")
    print(f"\nğŸŒ Contexto: {scenario['context']}")
    
    print("\nâ³ M.A.R.T.I.N. procesando...\n")
    
    result = agent.process(scenario['input'], scenario['context'])
    
    print(result['message'])
    print("\n" + "="*70)
    
    print("\nğŸ“Œ PUNTOS CLAVE DE ESTA DEMO:")
    for i, point in enumerate(scenario['talking_points'], 1):
        print(f"  {i}. {point}")
    
    print("\n" + "="*70 + "\n")
    
    return result

if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  M.A.R.T.I.N. - DEMOS PRE-PREPARADAS                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Presiona Enter para ejecutar cada demo...
    """)
    
    input("\nâ–¶ï¸  Demo 1: Modo Pasivo...")
    run_demo('scenario_1_passive')
    
    input("\nâ–¶ï¸  Demo 2: Modo Directo...")
    run_demo('scenario_2_direct')
    
    input("\nâ–¶ï¸  Demo 3: Modo Seguro...")
    run_demo('scenario_3_safe_blocked')
    
    print("\nâœ… Todas las demos completadas!")
```

---

### âœ… Checklist Fin del DÃ­a 3:

- [ ] Cloudflare Worker desplegado (bÃ¡sico)
- [ ] ComplianceEvaluator funcionando
- [ ] Tests de integraciÃ³n pasando
- [ ] Prompts optimizados
- [ ] 3 demos preparadas y testeadas 10+ veces
- [ ] Todo funciona sin fallos

**EstÃ¡s al 85% del MVP.** ğŸ‰

---

## ğŸ“… Jueves 23 Octubre - DÃA 4

### Objetivo del dÃ­a:
**DocumentaciÃ³n + PresentaciÃ³n + Backup**

---

### TODO EL DÃA (09:00 - 18:00) - 9 horas

#### 09:00 - 11:00 | README Profesional (2 horas)

**Archivo:** `README.md`

```markdown
# ğŸ§  M.A.R.T.I.N.

### Modular Assistant for Reasoning, Tactics, Inference and Navigation

---

## ğŸ¯ Â¿QuÃ© es M.A.R.T.I.N.?

M.A.R.T.I.N. es un **agente de IA con razonamiento adaptativo** que cambia su comportamiento segÃºn el contexto, diseÃ±ado para automatizaciÃ³n de compliance en startups.

### La InnovaciÃ³n: Razonamiento Tri-Modal

A diferencia de otros agentes que son siempre autÃ³nomos o siempre pasivos, M.A.R.T.I.N. **adapta su autonomÃ­a** al contexto:

- ğŸŸ¦ **MODO PASIVO**: Propone plan, espera confirmaciÃ³n (tareas ambiguas)
- ğŸŸ© **MODO DIRECTO**: Ejecuta autÃ³nomamente (tareas claras, bajo riesgo)
- ğŸŸ¨ **MODO SEGURO**: Auto-valida antes de actuar (alto riesgo, producciÃ³n)

---

## ğŸš€ Quick Start

```bash
# 1. Clonar repo
git clone https://github.com/tu-usuario/martin-agent
cd martin-agent

# 2. Setup
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 3. Configurar variables
cp .env.example .env
# Editar .env con tus API keys

# 4. Ejecutar
python ui/gradio_interface.py
```

Abre http://localhost:7860 en tu navegador.

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           USER INPUT                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  MODE SELECTOR  â”‚  â† Analiza riesgo y claridad
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                       â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ PASSIVE â”‚  â”‚   DIRECT   â”‚  â”‚  SAFE   â”‚
â”‚ REASON  â”‚  â”‚   REASON   â”‚  â”‚ REASON  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  TOOL EXECUTOR  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Casos de Uso

### 1. Compliance Automation
- AuditorÃ­as de infraestructura (GitHub, AWS)
- GeneraciÃ³n de polÃ­ticas (SOC 2, ISO 27001)
- EvaluaciÃ³n de gaps de compliance
- Roadmaps personalizados

### 2. Seguridad Proactiva
- DetecciÃ³n de secrets expuestos
- AnÃ¡lisis de configuraciones de riesgo
- PrevenciÃ³n de acciones destructivas

---

## ğŸ“Š Ejemplos

### Ejemplo 1: Modo Pasivo
```
Usuario: "AyÃºdame con SOC 2"

M.A.R.T.I.N. (Pasivo):
"Propongo este plan:
1. Auditar infraestructura (2h)
2. Identificar gaps crÃ­ticos (1h)
3. Generar roadmap (30min)

Â¿Procedo?"
```

### Ejemplo 2: Modo Directo
```
Usuario: "Genera polÃ­tica de passwords"

M.A.R.T.I.N. (Directo):
*Ejecuta inmediatamente*

"âœ… PolÃ­tica generada:
- Longitud mÃ­nima: 12 caracteres
- MFA obligatorio
- RotaciÃ³n: 90 dÃ­as
..."
```

### Ejemplo 3: Modo Seguro
```
Usuario: "Deshabilita admin@empresa.com"
Ambiente: Production

M.A.R.T.I.N. (Seguro):
"âš ï¸ VALIDACIÃ“N FALLIDA
Riesgos:
- Usuario con permisos root
- Puede causar pÃ©rdida de acceso
- No hay admin de respaldo

âŒ ACCIÃ“N BLOQUEADA
Alternativa: Crear admin temporal primero"
```

---

## ğŸ¯ Tech Stack

- **Core**: Python 3.10+, LangChain
- **LLM**: GPT-4 / Claude
- **Tools**: GitHub API, AWS SDK, Compliance frameworks
- **UI**: Gradio
- **Edge**: Cloudflare Workers
- **Testing**: pytest

---

## ğŸ“ Estructura del Proyecto

```
martin-agent/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ mode_selector.py       # Cerebro de selecciÃ³n
â”‚   â”œâ”€â”€ reasoning_engines.py   # 3 modos de razonamiento
â”‚   â””â”€â”€ martin_agent.py        # Orquestador principal
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ policy_generator.py    # Generador de polÃ­ticas
â”‚   â”œâ”€â”€ github_scanner.py      # Scanner de repos
â”‚   â””â”€â”€ compliance_evaluator.py # Evaluador SOC2/ISO
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ gradio_interface.py    # Interfaz web
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ integration_tests.py   # Tests E2E
â””â”€â”€ demos/
    â””â”€â”€ demo_scenarios.py      # Demos preparadas
```

---

## ğŸ§ª Testing

```bash
# Tests unitarios
python tests/test_mode_selector.py

# Tests de integraciÃ³n
python tests/integration_tests.py

# Ejecutar demos
python demos/demo_scenarios.py
```

---

## ğŸ† Â¿Por QuÃ© M.A.R.T.I.N. es Innovador?

### Otros agentes:
- âŒ AutoGPT: Siempre autÃ³nomo â†’ Peligroso en producciÃ³n
- âŒ ChatGPT: Siempre pasivo â†’ No es realmente autÃ³nomo
- âŒ Copilot: No valida riesgos â†’ Puede sugerir acciones destructivas

### M.A.R.T.I.N.:
- âœ… Adapta su autonomÃ­a al contexto
- âœ… Auto-valida en situaciones crÃ­ticas
- âœ… Transparente en su razonamiento
- âœ… Usuario mantiene control cuando necesita

---

## ğŸ“ License

MIT License - Ver [LICENSE](LICENSE)

---

## ğŸ‘¥ Equipo

Desarrollado para **The Agent Hackathon 2025** by Skyward.ai

---

## ğŸ™ Agradecimientos

- Skyward.ai por organizar el hackathon
- OpenAI/Anthropic por APIs de LLM
- Comunidad LangChain

---

**Built with ğŸ§  by [Tu Nombre]**
```

---

#### 11:00 - 13:00 | Video Demo (2 horas)

**Script del video (2 minutos):**

```
[0:00-0:15] INTRO
"Hola, soy [nombre] y les presento M.A.R.T.I.N.
Un agente de IA que no solo ejecuta tareas,
sino que PIENSA cÃ³mo ejecutarlas segÃºn el contexto."

[0:15-0:30] EL PROBLEMA
"Los agentes actuales son o muy autÃ³nomos (riesgoso)
o muy pasivos (ineficiente).
M.A.R.T.I.N. resuelve esto con razonamiento adaptativo."

[0:30-1:00] DEMO 1 - Modo Pasivo
[Mostrar pantalla]
"Usuario con necesidad ambigua...
M.A.R.T.I.N. detecta incertidumbre,
propone plan estructurado,
espera confirmaciÃ³n.
ColaboraciÃ³n humano-agente."

[1:00-1:20] DEMO 2 - Modo Directo
[Mostrar pantalla]
"Tarea clara: generar polÃ­tica.
M.A.R.T.I.N. ejecuta sin preguntar.
Resultado inmediato.
Explica su razonamiento."

[1:20-1:50] DEMO 3 - Modo Seguro
[Mostrar pantalla]
"AcciÃ³n peligrosa en producciÃ³n...
M.A.R.T.I.N. se AUTO-VALIDA,
detecta riesgos mÃºltiples,
BLOQUEA la acciÃ³n,
sugiere alternativa segura.
Previene errores costosos."

[1:50-2:00] CIERRE
"M.A.R.T.I.N.: Razonamiento adaptativo para IA segura.
Aplicado a compliance, pero extensible a cualquier dominio.
Gracias."
```

**Grabar con:**
- OBS Studio o Loom
- Calidad 1080p
- Audio claro
- Slides de apoyo si necesitas

---

#### 14:00 - 16:00 | Pitch Deck (2 horas)

**Archivo:** `docs/PITCH.md`

**5 Slides mÃ¡ximo:**

```markdown
# SLIDE 1: TÃTULO
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ§  M.A.R.T.I.N.
Razonamiento Adaptativo para Agentes de IA

The Agent Hackathon 2025
```

```markdown
# SLIDE 2: EL PROBLEMA
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âŒ Agentes actuales son rÃ­gidos:

AutoGPT: Siempre autÃ³nomo
â†’ Ejecuta sin validar
â†’ Riesgoso en producciÃ³n

ChatGPT: Siempre pasivo  
â†’ Pregunta todo
â†’ Ineficiente

ğŸ¯ Necesitamos agentes que ADAPTEN su autonomÃ­a
```

```markdown
# SLIDE 3: LA SOLUCIÃ“N
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
M.A.R.T.I.N. = Razonamiento Tri-Modal

ğŸŸ¦ PASIVO: Pregunta (tareas ambiguas)
ğŸŸ© DIRECTO: Ejecuta (tareas claras)
ğŸŸ¨ SEGURO: Valida (alto riesgo)

â†’ El MISMO agente se comporta diferente segÃºn contexto
```

```markdown
# SLIDE 4: CÃ“MO FUNCIONA
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
[Diagrama de arquitectura]

Input â†’ ModeSelector
      â†“
   Analiza riesgo + claridad
      â†“
Elige modo apropiado
      â†“
Reasoning Engine correspondiente
      â†“
EjecuciÃ³n (o no) segÃºn modo
```

```markdown
# SLIDE 5: IMPACTO
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… InnovaciÃ³n tÃ©cnica: Nadie tiene razonamiento tri-modal

âœ… AplicaciÃ³n real: Compliance para startups
   â†’ 50,000+ startups necesitan SOC 2
   â†’ M.A.R.T.I.N. reduce tiempo 80%
   â†’ Reduce costo de $50k a $5k

âœ… Extensible: El concepto aplica a cualquier dominio
   â†’ Ciberseguridad, finanzas, salud, etc.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"No es QUÃ‰ hace, sino CÃ“MO razona"
```

---

#### 16:00 - 18:00 | Backup Plan (2 horas)

**Crear:**

1. **Version offline** (sin APIs externas)
```python
# agents/offline_martin.py
# Version que funciona con respuestas pre-cargadas
# Para si falla internet en la demo
```

2. **Datos sintÃ©ticos** para demos
```python
# data/synthetic_data.py
# Organizaciones ficticias, repos de ejemplo, etc.
```

3. **Screenshots** de cada demo funcionando
```
docs/screenshots/
â”œâ”€â”€ demo1_passive.png
â”œâ”€â”€ demo2_direct.png
â””â”€â”€ demo3_safe.png
```

4. **USB con todo**
- CÃ³digo completo
- Video demo
- Slides PDF
- Screenshots
- Requirements.txt

---

### âœ… Checklist Fin del DÃ­a 4:

- [ ] README profesional completo
- [ ] Video demo de 2 minutos grabado
- [ ] Pitch deck con 5 slides
- [ ] Backup plan listo (offline mode)
- [ ] USB con todo preparado
- [ ] Ensayaste el pitch 5+ veces

**EstÃ¡s al 95% - CASI LISTO.** ğŸ”¥

---

## ğŸ“… Viernes 24 Octubre - DÃA 5

### Objetivo del dÃ­a:
**Ãšltimo pulido + Descanso**

---

### MAÃ‘ANA (09:00 - 12:00) - 3 horas

#### Ãšltimos ajustes:
- [ ] Revisar que todas las demos funcionen perfectamente
- [ ] Optimizar tiempos de respuesta
- [ ] Limpiar cÃ³digo (comentarios, prints de debug)
- [ ] Actualizar requirements.txt
- [ ] Push final a GitHub

---

### TARDE (13:00 - 17:00) - DESCANSO

**NO TRABAJES MÃS**

- Sal a caminar
- Haz ejercicio
- Ve una pelÃ­cula
- Duerme siesta
- RelÃ¡jate

---

### NOCHE (18:00 - 22:00) - PreparaciÃ³n final

- [ ] Ensayo final completo (3 veces)
- [ ] Verificar que laptop estÃ© cargada
- [ ] Preparar mochila:
  * Laptop + cargador
  * Mouse
  * USB con backup
  * Botella de agua
  * Snacks
  * Audifonos
- [ ] Dormir temprano (23:00)